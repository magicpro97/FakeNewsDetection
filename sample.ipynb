{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-27T00:01:16.216136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set CPU threading configuration BEFORE importing TensorFlow\n",
    "num_physical_cores = os.cpu_count()\n",
    "num_logical_cores = os.cpu_count()\n",
    "if num_physical_cores is not None:\n",
    "    # These environment variables need to be set before TensorFlow is imported\n",
    "    os.environ[\"TF_NUM_INTEROP_THREADS\"] = str(num_physical_cores)\n",
    "    os.environ[\"TF_NUM_INTRAOP_THREADS\"] = str(num_logical_cores)\n",
    "    print(f\"üí• CPU Threading configured: {num_physical_cores} physical cores, {num_logical_cores} logical cores\")\n",
    "\n",
    "# GPU diagnostic code - Add before TensorFlow import\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_nvidia_gpu():\n",
    "    try:\n",
    "        nvidia_output = subprocess.check_output(['nvidia-smi'], stderr=subprocess.STDOUT).decode('utf-8')\n",
    "        print(\"üí• NVIDIA-SMI Output:\")\n",
    "        print(nvidia_output)\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"‚ùå nvidia-smi command failed - NVIDIA driver may not be properly installed\")\n",
    "        return False\n",
    "\n",
    "def check_cuda_installation():\n",
    "    try:\n",
    "        nvcc_output = subprocess.check_output(['nvcc', '--version'], stderr=subprocess.STDOUT).decode('utf-8')\n",
    "        print(\"üí• CUDA Compiler Version:\")\n",
    "        print(nvcc_output)\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"‚ùå CUDA toolkit not found in PATH - CUDA may not be properly installed\")\n",
    "        return False\n",
    "\n",
    "print(\"\\nüí• CHECKING GPU PREREQUISITES:\")\n",
    "has_nvidia_driver = check_nvidia_gpu()\n",
    "has_cuda = check_cuda_installation()\n",
    "\n",
    "# Check Python and TensorFlow paths (to detect potential environment issues)\n",
    "print(f\"\\nüí• Python executable: {sys.executable}\")\n",
    "print(f\"üí• Python version: {sys.version}\")\n",
    "\n",
    "# Now import TensorFlow after setting thread configurations\n",
    "import tensorflow as tf\n",
    "print(f\"\\nüí• TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üí• TensorFlow built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "print(f\"üí• TensorFlow GPU available: {tf.test.is_gpu_available()}\")\n",
    "\n",
    "if has_nvidia_driver and not tf.test.is_gpu_available():\n",
    "    print(\"\\n‚ùó POTENTIAL ISSUE DETECTED:\")\n",
    "    print(\"   - NVIDIA GPU detected by system but not by TensorFlow\")\n",
    "    print(\"   - This may be caused by:\")\n",
    "    print(\"     1. Using CPU-only TensorFlow instead of GPU version\")\n",
    "    print(\"     2. Incompatible CUDA or cuDNN versions\")\n",
    "    print(\"     3. Environment configuration issues\")\n",
    "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "    print(\"   - Ensure you have installed tensorflow-gpu or tensorflow>=2.1 with pip\")\n",
    "    print(\"   - Check compatible CUDA/cuDNN versions for your TensorFlow version\")\n",
    "    print(\"   - Try: pip install tensorflow==2.10.0 (or another recent version)\")\n",
    "    print(\"   - For manual GPU setup, see: https://www.tensorflow.org/install/gpu\")\n",
    "\n",
    "# üöÄ **Ki·ªÉm tra v√† c·∫•u h√¨nh CPU**\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"üí• Detected {len(gpus)} GPU(s):\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"  GPU {i}: {gpu.name}\")\n",
    "        \n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"üí• {len(logical_gpus)} Logical GPU(s) available\")\n",
    "        \n",
    "        # üöÄ **B·∫≠t Mixed Precision cho GPU ƒë·ªÉ tƒÉng t·ªëc**\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "        print(f\"üí• Mixed precision policy: {tf.keras.mixed_precision.global_policy()}\")\n",
    "        \n",
    "        # üöÄ **K√≠ch ho·∫°t XLA compiler ƒë·ªÉ tƒÉng hi·ªáu su·∫•t tr√™n GPU**\n",
    "        tf.config.optimizer.set_jit(True)\n",
    "        print(\"üí• XLA JIT compilation enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ùå GPU error: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected by TensorFlow. Running on CPU.\")\n",
    "    # Configure for CPU - use float32 for better compatibility\n",
    "    tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "    print(f\"üí• Mixed precision policy: {tf.keras.mixed_precision.global_policy()}\")\n",
    "    print(f\"üí• CPU Optimization: Using {os.environ.get('TF_NUM_INTEROP_THREADS')} inter-op threads, {os.environ.get('TF_NUM_INTRAOP_THREADS')} intra-op threads\")\n",
    "        \n",
    "    # Disable XLA which isn't needed for CPU\n",
    "    tf.config.optimizer.set_jit(False)\n",
    "    print(\"üí• XLA JIT compilation disabled for CPU\")\n",
    "\n",
    "# CPU memory monitor function\n",
    "def cpu_memory_usage():\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_info = process.memory_info()\n",
    "        return f\"Memory usage: {memory_info.rss / (1024 * 1024):.1f} MB\"\n",
    "    except:\n",
    "        return \"Memory monitoring not available\"\n",
    "\n",
    "# Resource monitoring function that works for both CPU and GPU\n",
    "def resource_usage():\n",
    "    if gpus:\n",
    "        try:\n",
    "            import subprocess\n",
    "            result = subprocess.check_output(\n",
    "                ['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader']\n",
    "            ).decode('utf-8')\n",
    "            memory_usage = [tuple(map(int, x.split(','))) for x in result.strip().split('\\n')]\n",
    "            return [f\"GPU {i}: {used} MB / {total} MB ({used/total:.1%})\" for i, (used, total) in enumerate(memory_usage)]\n",
    "        except:\n",
    "            return [\"GPU memory monitoring not available\"]\n",
    "    else:\n",
    "        return [cpu_memory_usage()]\n",
    "\n",
    "# üî• **Load PhoBERT tokenizer v√† model**\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert = TFAutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert.trainable = False  # ‚ö° ƒê√≥ng bƒÉng PhoBERT\n",
    "\n",
    "# Print memory after loading model\n",
    "print(\"\\nüí• Memory after loading PhoBERT:\")\n",
    "print(resource_usage())\n",
    "\n",
    "# üöÄ **H√†m ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu**\n",
    "def preprocess_data(texts, labels):\n",
    "    # Convert to list if they're Series objects\n",
    "    if hasattr(texts, 'tolist'):\n",
    "        texts = texts.tolist()\n",
    "    if hasattr(labels, 'tolist'):\n",
    "        labels = labels.tolist()\n",
    "\n",
    "    # Clean the text data - ensure all are strings\n",
    "    cleaned_texts = []\n",
    "    cleaned_labels = []\n",
    "    \n",
    "    for i, (text, label) in enumerate(zip(texts, labels)):\n",
    "        # Skip None or NaN values\n",
    "        if text is None or (isinstance(text, float) and pd.isna(text)):\n",
    "            print(f\"Warning: Skipping item {i} with None/NaN text\")\n",
    "            continue\n",
    "            \n",
    "        # Convert to string if not already\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            \n",
    "        cleaned_texts.append(text)\n",
    "        cleaned_labels.append(label)\n",
    "    \n",
    "    if not cleaned_texts:\n",
    "        raise ValueError(\"No valid text entries found after cleaning\")\n",
    "        \n",
    "    # Tokenize vƒÉn b·∫£n v·ªõi PhoBERT\n",
    "    inputs = tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=256, return_tensors='tf')\n",
    "\n",
    "    return tf.convert_to_tensor(inputs['input_ids'], dtype=tf.int32), \\\n",
    "           tf.convert_to_tensor(inputs['attention_mask'], dtype=tf.int32), \\\n",
    "           tf.convert_to_tensor(cleaned_labels, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# üî• **T·∫°o l·ªõp Keras t√πy ch·ªânh cho PhoBERT**\n",
    "class CustomPhoBERTLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, phobert_model, **kwargs):\n",
    "        super(CustomPhoBERTLayer, self).__init__(**kwargs)\n",
    "        self.phobert = phobert_model\n",
    "        self.phobert_name = \"vinai/phobert-base\"  # Store the name for serialization\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        output = self.phobert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        return output\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super(CustomPhoBERTLayer, self).get_config()\n",
    "        config.update({\"phobert_name\": self.phobert_name})\n",
    "        return config\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Load PhoBERT when reconstructing the layer\n",
    "        from transformers import TFAutoModel\n",
    "        config_copy = dict(config)\n",
    "        phobert_name = config_copy.pop(\"phobert_name\")\n",
    "        phobert_model = TFAutoModel.from_pretrained(phobert_name)\n",
    "        phobert_model.trainable = False\n",
    "        return cls(phobert_model, **config_copy)\n",
    "\n",
    "\n",
    "# üî• **H√†m x√¢y d·ª±ng m√¥ h√¨nh PhoBERT**\n",
    "def build_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # ‚úÖ **D√πng l·ªõp CustomPhoBERTLayer thay v√¨ Lambda**\n",
    "    phobert_output = CustomPhoBERTLayer(phobert)([input_ids, attention_mask])\n",
    "\n",
    "    # üìå **L·∫•y embedding t·ª´ token ƒë·∫ßu ti√™n [CLS]**\n",
    "    text_embedding = tf.keras.layers.Lambda(lambda x: x[:, 0, :])(phobert_output)\n",
    "\n",
    "    dropout = tf.keras.layers.Dropout(0.1)(text_embedding)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# üöÄ **Load d·ªØ li·ªáu**\n",
    "real_news = pd.read_csv('./data/vnexpress_dataset.csv')\n",
    "fake_news = pd.read_csv('./data/vnexpress_fake_dataset.csv')\n",
    "\n",
    "# G√°n nh√£n\n",
    "real_news['Label'] = 0\n",
    "fake_news['Label'] = 1\n",
    "data = pd.concat([real_news, fake_news], ignore_index=True)\n",
    "\n",
    "# üöÄ **Chia th√†nh train (70%), validation (15%) v√† test (15%)**\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['Content'], data['Label'], test_size=0.3, random_state=42, stratify=data['Label']\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    test_texts, test_labels, test_size=0.5, random_state=42, stratify=test_labels\n",
    ")\n",
    "\n",
    "# üöÄ **Tokenize d·ªØ li·ªáu**\n",
    "train_inputs, train_mask, train_labels = preprocess_data(train_texts, train_labels)\n",
    "val_inputs, val_mask, val_labels = preprocess_data(val_texts, val_labels)\n",
    "test_inputs, test_mask, test_labels = preprocess_data(test_texts, test_labels)\n",
    "\n",
    "# üöÄ **T·∫°o dataset TensorFlow**\n",
    "# Smaller batch size for CPU\n",
    "batch_size = 8 if not gpus else 16\n",
    "\n",
    "# Optimize datasets for performance\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'input_ids': train_inputs, 'attention_mask': train_mask}, train_labels)) \\\n",
    "    .cache() \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_inputs, 'attention_mask': val_mask}, val_labels)) \\\n",
    "    .cache() \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'input_ids': test_inputs, 'attention_mask': test_mask}, test_labels)) \\\n",
    "    .cache() \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# üöÄ **Train model**\n",
    "model = build_model()\n",
    "\n",
    "# Create callback to monitor resource usage\n",
    "class ResourceMonitor(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\nüí• Resource usage before epoch {epoch+1}:\")\n",
    "        print(resource_usage())\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\nüí• Resource usage after epoch {epoch+1}:\")\n",
    "        print(resource_usage())\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train with monitoring\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    validation_data=val_dataset, \n",
    "    epochs=3,\n",
    "    callbacks=[ResourceMonitor()]\n",
    ")\n",
    "\n",
    "# Print training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nüí• Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# üöÄ **ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test**\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ],
   "id": "c775f6091bb10f97",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí• CPU Threading configured: 16 physical cores, 16 logical cores\n",
      "\n",
      "üí• CHECKING GPU PREREQUISITES:\n",
      "üí• NVIDIA-SMI Output:\n",
      "Thu Mar 27 07:01:16 2025       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 572.61                 Driver Version: 572.61         CUDA Version: 12.8     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3080 Ti   WDDM  |   00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   42C    P8             36W /  400W |    2770MiB /  12288MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|    0   N/A  N/A            1280    C+G   ...4__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\r\n",
      "|    0   N/A  N/A            1752    C+G   ....0.3124.85\\msedgewebview2.exe      N/A      |\r\n",
      "|    0   N/A  N/A            2168    C+G   ...xyewy\\ShellExperienceHost.exe      N/A      |\r\n",
      "|    0   N/A  N/A            2176    C+G   ...aries\\Win64\\EpicWebHelper.exe      N/A      |\r\n",
      "|    0   N/A  N/A            3680    C+G   ...64__zpdnekdrzrea0\\Spotify.exe      N/A      |\r\n",
      "|    0   N/A  N/A            4180    C+G   ...t\\Edge\\Application\\msedge.exe      N/A      |\r\n",
      "|    0   N/A  N/A           10848    C+G   C:\\Windows\\explorer.exe               N/A      |\r\n",
      "|    0   N/A  N/A           11924    C+G   ...IA app\\CEF\\NVIDIA Overlay.exe      N/A      |\r\n",
      "|    0   N/A  N/A           12844    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\r\n",
      "|    0   N/A  N/A           13016    C+G   ...IA app\\CEF\\NVIDIA Overlay.exe      N/A      |\r\n",
      "|    0   N/A  N/A           13132    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\r\n",
      "|    0   N/A  N/A           13484      C   ...grams\\LM Studio\\LM Studio.exe      N/A      |\r\n",
      "|    0   N/A  N/A           14184    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\r\n",
      "|    0   N/A  N/A           14248    C+G   ...cord\\app-1.0.9186\\Discord.exe      N/A      |\r\n",
      "|    0   N/A  N/A           14860    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\r\n",
      "|    0   N/A  N/A           15092    C+G   ...ef.win7x64\\steamwebhelper.exe      N/A      |\r\n",
      "|    0   N/A  N/A           15544    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\r\n",
      "|    0   N/A  N/A           18232    C+G   ...crosoft OneDrive\\OneDrive.exe      N/A      |\r\n",
      "|    0   N/A  N/A           18240    C+G   ...box\\bin\\jetbrains-toolbox.exe      N/A      |\r\n",
      "|    0   N/A  N/A           18468    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\r\n",
      "|    0   N/A  N/A           19020    C+G   ...rofessional\\bin\\pycharm64.exe      N/A      |\r\n",
      "|    0   N/A  N/A           19672    C+G   ...s\\Win64\\EpicGamesLauncher.exe      N/A      |\r\n",
      "|    0   N/A  N/A           20908    C+G   ...gato\\CameraHub\\Camera Hub.exe      N/A      |\r\n",
      "|    0   N/A  N/A           22332    C+G   ...Sony\\INZONE Hub\\INZONEHub.exe      N/A      |\r\n",
      "|    0   N/A  N/A           23144    C+G   ...App_cw5n1h2txyewy\\LockApp.exe      N/A      |\r\n",
      "|    0   N/A  N/A           23416      C   ...grams\\LM Studio\\LM Studio.exe      N/A      |\r\n",
      "|    0   N/A  N/A           25188    C+G   ...grams\\LM Studio\\LM Studio.exe      N/A      |\r\n",
      "|    0   N/A  N/A           25772      C   ...grams\\LM Studio\\LM Studio.exe      N/A      |\r\n",
      "|    0   N/A  N/A           25924    C+G   ...4__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\r\n",
      "|    0   N/A  N/A           26516    C+G   ....0.3124.85\\msedgewebview2.exe      N/A      |\r\n",
      "|    0   N/A  N/A           27956    C+G   ...2p2nqsd0c76g0\\app\\ChatGPT.exe      N/A      |\r\n",
      "|    0   N/A  N/A           31320    C+G   ...t\\Edge\\Application\\msedge.exe      N/A      |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "\n",
      "üí• CUDA Compiler Version:\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\r\n",
      "Built on Fri_Feb_21_20:42:46_Pacific_Standard_Time_2025\r\n",
      "Cuda compilation tools, release 12.8, V12.8.93\r\n",
      "Build cuda_12.8.r12.8/compiler.35583870_0\r\n",
      "\n",
      "\n",
      "üí• Python executable: C:\\Users\\Linh Ngo\\PycharmProjects\\FakeNewsDetection\\.venv\\Scripts\\python.exe\n",
      "üí• Python version: 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]\n",
      "\n",
      "üí• TensorFlow version: 2.19.0\n",
      "üí• TensorFlow built with CUDA: False\n",
      "üí• TensorFlow GPU available: False\n",
      "\n",
      "‚ùó POTENTIAL ISSUE DETECTED:\n",
      "   - NVIDIA GPU detected by system but not by TensorFlow\n",
      "   - This may be caused by:\n",
      "     1. Using CPU-only TensorFlow instead of GPU version\n",
      "     2. Incompatible CUDA or cuDNN versions\n",
      "     3. Environment configuration issues\n",
      "\n",
      "üí° RECOMMENDATIONS:\n",
      "   - Ensure you have installed tensorflow-gpu or tensorflow>=2.1 with pip\n",
      "   - Check compatible CUDA/cuDNN versions for your TensorFlow version\n",
      "   - Try: pip install tensorflow==2.10.0 (or another recent version)\n",
      "   - For manual GPU setup, see: https://www.tensorflow.org/install/gpu\n",
      "‚ùå No GPU detected by TensorFlow. Running on CPU.\n",
      "üí• Mixed precision policy: <DTypePolicy \"float32\">\n",
      "üí• CPU Optimization: Using 16 inter-op threads, 16 intra-op threads\n",
      "üí• XLA JIT compilation disabled for CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí• Memory after loading PhoBERT:\n",
      "['Memory usage: 3890.5 MB']\n",
      "\n",
      "üí• Resource usage before epoch 1:\n",
      "['Memory usage: 3801.4 MB']\n",
      "Epoch 1/3\n",
      "\u001B[1m 12/896\u001B[0m \u001B[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[1m14:43\u001B[0m 999ms/step - accuracy: 0.2448 - loss: 0.8557"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2fa3160fdfe9dc6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# üíæ L∆∞u m√¥ h√¨nh (Fixed model saving)\n",
    "try:\n",
    "    model.save('./model/fake_news_model.keras')\n",
    "    print(\"‚úÖ Model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n",
    "    # Alternative saving method\n",
    "    print(\"Trying alternative save method...\")\n",
    "    model.save_weights('./model/fake_news_model.weights.h5')\n",
    "    print(\"‚úÖ Model weights saved successfully\")\n"
   ],
   "id": "cf01973d77576bda"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# üìä T·∫°o h√†m ƒë·ªÉ d·ª± ƒëo√°n tin t·ª©c m·ªõi\n",
    "def predict_news(text, model, tokenizer):\n",
    "    # ƒê·∫£m b·∫£o text l√† string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # Tokenize vƒÉn b·∫£n v·ªõi PhoBERT\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=256, return_tensors='tf')\n",
    "    \n",
    "    # D·ª± ƒëo√°n\n",
    "    prediction = model.predict({\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask']\n",
    "    }, verbose=0)\n",
    "    \n",
    "    probability = prediction[0][0]\n",
    "    \n",
    "    # Di·ªÖn gi·∫£i k·∫øt qu·∫£\n",
    "    result = \"FAKE\" if probability >= 0.5 else \"REAL\"\n",
    "    confidence = probability if probability >= 0.5 else 1 - probability\n",
    "    \n",
    "    return {\n",
    "        'result': result,\n",
    "        'confidence': float(confidence),\n",
    "        'probability': float(probability)\n",
    "    }\n",
    "\n",
    "# üì± Load m√¥ h√¨nh ƒë√£ l∆∞u\n",
    "def load_model_for_inference():\n",
    "    try:\n",
    "        # Define CustomPhoBERTLayer again for model loading\n",
    "        class CustomPhoBERTLayer(tf.keras.layers.Layer):\n",
    "            def __init__(self, phobert_model, **kwargs):\n",
    "                super(CustomPhoBERTLayer, self).__init__(**kwargs)\n",
    "                self.phobert = phobert_model\n",
    "                self.phobert_name = \"vinai/phobert-base\"\n",
    "\n",
    "            def call(self, inputs):\n",
    "                input_ids, attention_mask = inputs\n",
    "                output = self.phobert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "                return output\n",
    "                \n",
    "            def get_config(self):\n",
    "                config = super(CustomPhoBERTLayer, self).get_config()\n",
    "                config.update({\"phobert_name\": self.phobert_name})\n",
    "                return config\n",
    "                \n",
    "            @classmethod\n",
    "            def from_config(cls, config):\n",
    "                from transformers import TFAutoModel\n",
    "                config_copy = dict(config)\n",
    "                phobert_name = config_copy.pop(\"phobert_name\")\n",
    "                phobert_model = TFAutoModel.from_pretrained(phobert_name)\n",
    "                phobert_model.trainable = False\n",
    "                return cls(phobert_model, **config_copy)\n",
    "                \n",
    "        # Th·ª≠ load full model\n",
    "        loaded_model = tf.keras.models.load_model(\n",
    "            './model/fake_news_model.keras',\n",
    "            custom_objects={'CustomPhoBERTLayer': CustomPhoBERTLayer}\n",
    "        )\n",
    "        print(\"‚úÖ Model loaded successfully\")\n",
    "        return loaded_model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading full model: {e}\")\n",
    "        print(\"Loading model from architecture and weights...\")\n",
    "        \n",
    "        # N·∫øu kh√¥ng th√†nh c√¥ng, t·∫°o l·∫°i model v√† load weights\n",
    "        new_model = build_model()\n",
    "        new_model.load_weights('./model/fake_news_model.weights.h5')\n",
    "        print(\"‚úÖ Model loaded from weights successfully\")\n",
    "        return new_model\n",
    "\n",
    "# üîç Test v·ªõi m·ªôt s·ªë v√≠ d·ª•\n",
    "try:\n",
    "    # Load model\n",
    "    inference_model = load_model_for_inference()\n",
    "    \n",
    "    # V√≠ d·ª• v·ªÅ tin th·∫≠t\n",
    "    real_news_example = \"Th·ªß t∆∞·ªõng Ph·∫°m Minh Ch√≠nh cho bi·∫øt Vi·ªát Nam lu√¥n coi tr·ªçng h·ª£p t√°c v·ªõi EU v√† ƒë·ªÅ ngh·ªã EU s·ªõm ho√†n t·∫•t ph√™ chu·∫©n Hi·ªáp ƒë·ªãnh B·∫£o h·ªô ƒë·∫ßu t∆∞ Vi·ªát Nam-EU.\"\n",
    "    \n",
    "    # V√≠ d·ª• v·ªÅ tin gi·∫£\n",
    "    fake_news_example = \"Nh√† khoa h·ªçc Vi·ªát Nam ch·∫ø t·∫°o th√†nh c√¥ng m√°y ph√°t ƒëi·ªán vƒ©nh c·ª≠u kh√¥ng c·∫ßn nhi√™n li·ªáu, c√≥ th·ªÉ cung c·∫•p ƒëi·ªán mi·ªÖn ph√≠ cho to√†n b·ªô ƒë·∫•t n∆∞·ªõc.\"\n",
    "    \n",
    "    # D·ª± ƒëo√°n\n",
    "    real_result = predict_news(real_news_example, inference_model, tokenizer)\n",
    "    fake_result = predict_news(fake_news_example, inference_model, tokenizer)\n",
    "    \n",
    "    # In k·∫øt qu·∫£\n",
    "    print(\"\\nüîç KI·ªÇM TRA TIN TH·∫¨T:\")\n",
    "    print(f\"N·ªôi dung: {real_news_example}\")\n",
    "    print(f\"K·∫øt qu·∫£: {real_result['result']} (ƒë·ªô tin c·∫≠y: {real_result['confidence']:.2%})\")\n",
    "    \n",
    "    print(\"\\nüîç KI·ªÇM TRA TIN GI·∫¢:\")\n",
    "    print(f\"N·ªôi dung: {fake_news_example}\")\n",
    "    print(f\"K·∫øt qu·∫£: {fake_result['result']} (ƒë·ªô tin c·∫≠y: {fake_result['confidence']:.2%})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during inference: {e}\")\n",
    "\n",
    "\n"
   ],
   "id": "c3076427f728ad74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
