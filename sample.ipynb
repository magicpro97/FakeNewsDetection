{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e84a91ec4807ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:58:14.741547Z",
     "start_time": "2025-03-27T16:03:42.068322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "üí• CPU Threading configured: 28 physical cores, 28 logical cores\n",
      "\n",
      "üí• CHECKING GPU PREREQUISITES:\n",
      "üí• NVIDIA-SMI Output:\n",
      "Fri Mar 28 19:18:34 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.42                 Driver Version: 572.42         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   47C    P8              2W /   53W |     177MiB /   8188MiB |      8%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            3036    C+G   ...gram Files\\Parsec\\parsecd.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "‚ùå CUDA toolkit not found in PATH - CUDA may not be properly installed\n",
      "\n",
      "üí• Python executable: c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\n",
      "üí• Python version: 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\n",
      "\n",
      "üí• TensorFlow version: 2.19.0\n",
      "üí• TensorFlow built with CUDA: False\n",
      "WARNING:tensorflow:From C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_17988\\3262594074.py:54: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "üí• TensorFlow GPU available: False\n",
      "\n",
      "‚ùó POTENTIAL ISSUE DETECTED:\n",
      "   - NVIDIA GPU detected by system but not by TensorFlow\n",
      "   - This may be caused by:\n",
      "     1. Using CPU-only TensorFlow instead of GPU version\n",
      "     2. Incompatible CUDA or cuDNN versions\n",
      "     3. Environment configuration issues\n",
      "\n",
      "üí° RECOMMENDATIONS:\n",
      "   - Ensure you have installed tensorflow-gpu or tensorflow>=2.1 with pip\n",
      "   - Check compatible CUDA/cuDNN versions for your TensorFlow version\n",
      "   - Try: pip install tensorflow==2.10.0 (or another recent version)\n",
      "   - For manual GPU setup, see: https://www.tensorflow.org/install/gpu\n",
      "‚ùå No GPU detected by TensorFlow. Running on CPU.\n",
      "üí• Mixed precision policy: <DTypePolicy \"float32\">\n",
      "üí• CPU Optimization: Using 28 inter-op threads, 28 intra-op threads\n",
      "üí• XLA JIT compilation disabled for CPU\n",
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí• Memory after loading PhoBERT:\n",
      "['Memory usage: 2005.2 MB']\n",
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      "üí• Resource usage before epoch 1:\n",
      "['Memory usage: 2075.9 MB']\n",
      "Epoch 1/3\n",
      "\u001b[1m896/896\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 910ms/step - accuracy: 0.5402 - loss: 0.6749\n",
      "üí• Resource usage after epoch 1:\n",
      "['Memory usage: 2228.2 MB']\n",
      "\u001b[1m896/896\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m988s\u001b[0m 1s/step - accuracy: 0.5404 - loss: 0.6748 - val_accuracy: 0.8365 - val_loss: 0.4494\n",
      "\n",
      "üí• Resource usage before epoch 2:\n",
      "['Memory usage: 2228.2 MB']\n",
      "Epoch 2/3\n",
      "\u001b[1m896/896\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935ms/step - accuracy: 0.8380 - loss: 0.4484\n",
      "üí• Resource usage after epoch 2:\n",
      "['Memory usage: 1037.4 MB']\n",
      "\u001b[1m896/896\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m998s\u001b[0m 1s/step - accuracy: 0.8380 - loss: 0.4484 - val_accuracy: 0.8365 - val_loss: 0.4455\n",
      "\n",
      "üí• Resource usage before epoch 3:\n",
      "['Memory usage: 1037.4 MB']\n",
      "Epoch 3/3\n",
      "\u001b[1m395/896\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m7:35\u001b[0m 908ms/step - accuracy: 0.8390 - loss: 0.4453"
     ]
    }
   ],
   "source": [
    "from tf_keras.src.saving.object_registration import register_keras_serializable\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Set CPU threading configuration BEFORE importing TensorFlow\n",
    "num_physical_cores = os.cpu_count()\n",
    "num_logical_cores = os.cpu_count()\n",
    "if num_physical_cores is not None:\n",
    "    # These environment variables need to be set before TensorFlow is imported\n",
    "    os.environ[\"TF_NUM_INTEROP_THREADS\"] = str(num_physical_cores)\n",
    "    os.environ[\"TF_NUM_INTRAOP_THREADS\"] = str(num_logical_cores)\n",
    "    print(f\"üí• CPU Threading configured: {num_physical_cores} physical cores, {num_logical_cores} logical cores\")\n",
    "\n",
    "# GPU diagnostic code - Add before TensorFlow import\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def check_nvidia_gpu():\n",
    "    try:\n",
    "        nvidia_output = subprocess.check_output(['nvidia-smi'], stderr=subprocess.STDOUT).decode('utf-8')\n",
    "        print(\"üí• NVIDIA-SMI Output:\")\n",
    "        print(nvidia_output)\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"‚ùå nvidia-smi command failed - NVIDIA driver may not be properly installed\")\n",
    "        return False\n",
    "\n",
    "def check_cuda_installation():\n",
    "    try:\n",
    "        nvcc_output = subprocess.check_output(['nvcc', '--version'], stderr=subprocess.STDOUT).decode('utf-8')\n",
    "        print(\"üí• CUDA Compiler Version:\")\n",
    "        print(nvcc_output)\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        print(\"‚ùå CUDA toolkit not found in PATH - CUDA may not be properly installed\")\n",
    "        return False\n",
    "\n",
    "print(\"\\nüí• CHECKING GPU PREREQUISITES:\")\n",
    "has_nvidia_driver = check_nvidia_gpu()\n",
    "has_cuda = check_cuda_installation()\n",
    "\n",
    "# Check Python and TensorFlow paths (to detect potential environment issues)\n",
    "print(f\"\\nüí• Python executable: {sys.executable}\")\n",
    "print(f\"üí• Python version: {sys.version}\")\n",
    "\n",
    "# Now import TensorFlow after setting thread configurations\n",
    "import tensorflow as tf\n",
    "print(f\"\\nüí• TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üí• TensorFlow built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "print(f\"üí• TensorFlow GPU available: {tf.test.is_gpu_available()}\")\n",
    "\n",
    "if has_nvidia_driver and not tf.test.is_gpu_available():\n",
    "    print(\"\\n‚ùó POTENTIAL ISSUE DETECTED:\")\n",
    "    print(\"   - NVIDIA GPU detected by system but not by TensorFlow\")\n",
    "    print(\"   - This may be caused by:\")\n",
    "    print(\"     1. Using CPU-only TensorFlow instead of GPU version\")\n",
    "    print(\"     2. Incompatible CUDA or cuDNN versions\")\n",
    "    print(\"     3. Environment configuration issues\")\n",
    "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "    print(\"   - Ensure you have installed tensorflow-gpu or tensorflow>=2.1 with pip\")\n",
    "    print(\"   - Check compatible CUDA/cuDNN versions for your TensorFlow version\")\n",
    "    print(\"   - Try: pip install tensorflow==2.10.0 (or another recent version)\")\n",
    "    print(\"   - For manual GPU setup, see: https://www.tensorflow.org/install/gpu\")\n",
    "\n",
    "# üöÄ **Ki·ªÉm tra v√† c·∫•u h√¨nh CPU**\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"üí• Detected {len(gpus)} GPU(s):\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"  GPU {i}: {gpu.name}\")\n",
    "\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"üí• {len(logical_gpus)} Logical GPU(s) available\")\n",
    "\n",
    "        # üöÄ **B·∫≠t Mixed Precision cho GPU ƒë·ªÉ tƒÉng t·ªëc**\n",
    "        tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "        print(f\"üí• Mixed precision policy: {tf.keras.mixed_precision.global_policy()}\")\n",
    "\n",
    "        # üöÄ **K√≠ch ho·∫°t XLA compiler ƒë·ªÉ tƒÉng hi·ªáu su·∫•t tr√™n GPU**\n",
    "        tf.config.optimizer.set_jit(True)\n",
    "        print(\"üí• XLA JIT compilation enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ùå GPU error: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected by TensorFlow. Running on CPU.\")\n",
    "    # Configure for CPU - use float32 for better compatibility\n",
    "    tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "    print(f\"üí• Mixed precision policy: {tf.keras.mixed_precision.global_policy()}\")\n",
    "    print(f\"üí• CPU Optimization: Using {os.environ.get('TF_NUM_INTEROP_THREADS')} inter-op threads, {os.environ.get('TF_NUM_INTRAOP_THREADS')} intra-op threads\")\n",
    "\n",
    "    # Disable XLA which isn't needed for CPU\n",
    "    tf.config.optimizer.set_jit(False)\n",
    "    print(\"üí• XLA JIT compilation disabled for CPU\")\n",
    "\n",
    "# CPU memory monitor function\n",
    "def cpu_memory_usage():\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_info = process.memory_info()\n",
    "        return f\"Memory usage: {memory_info.rss / (1024 * 1024):.1f} MB\"\n",
    "    except:\n",
    "        return \"Memory monitoring not available\"\n",
    "\n",
    "# Resource monitoring function that works for both CPU and GPU\n",
    "def resource_usage():\n",
    "    if gpus:\n",
    "        try:\n",
    "            import subprocess\n",
    "            result = subprocess.check_output(\n",
    "                ['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,nounits,noheader']\n",
    "            ).decode('utf-8')\n",
    "            memory_usage = [tuple(map(int, x.split(','))) for x in result.strip().split('\\n')]\n",
    "            return [f\"GPU {i}: {used} MB / {total} MB ({used/total:.1%})\" for i, (used, total) in enumerate(memory_usage)]\n",
    "        except:\n",
    "            return [\"GPU memory monitoring not available\"]\n",
    "    else:\n",
    "        return [cpu_memory_usage()]\n",
    "\n",
    "# üî• **Load PhoBERT tokenizer v√† model**\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert = TFAutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert.trainable = False  # ‚ö° ƒê√≥ng bƒÉng PhoBERT\n",
    "\n",
    "# Print memory after loading model\n",
    "print(\"\\nüí• Memory after loading PhoBERT:\")\n",
    "print(resource_usage())\n",
    "\n",
    "# üöÄ **H√†m ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu**\n",
    "def preprocess_data(texts, labels):\n",
    "    # Convert to list if they're Series objects\n",
    "    if hasattr(texts, 'tolist'):\n",
    "        texts = texts.tolist()\n",
    "    if hasattr(labels, 'tolist'):\n",
    "        labels = labels.tolist()\n",
    "\n",
    "    # Clean the text data - ensure all are strings\n",
    "    cleaned_texts = []\n",
    "    cleaned_labels = []\n",
    "\n",
    "    for i, (text, label) in enumerate(zip(texts, labels)):\n",
    "        # Skip None or NaN values\n",
    "        if text is None or (isinstance(text, float) and pd.isna(text)):\n",
    "            print(f\"Warning: Skipping item {i} with None/NaN text\")\n",
    "            continue\n",
    "\n",
    "        # Convert to string if not already\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "\n",
    "        cleaned_texts.append(text)\n",
    "        cleaned_labels.append(label)\n",
    "\n",
    "    if not cleaned_texts:\n",
    "        raise ValueError(\"No valid text entries found after cleaning\")\n",
    "\n",
    "    # Tokenize vƒÉn b·∫£n v·ªõi PhoBERT\n",
    "    inputs = tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=256, return_tensors='tf')\n",
    "\n",
    "    return tf.convert_to_tensor(inputs['input_ids'], dtype=tf.int32), \\\n",
    "           tf.convert_to_tensor(inputs['attention_mask'], dtype=tf.int32), \\\n",
    "           tf.convert_to_tensor(cleaned_labels, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# üî• **T·∫°o l·ªõp Keras t√πy ch·ªânh cho PhoBERT**\n",
    "@register_keras_serializable()\n",
    "class CustomPhoBERTLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, phobert_model=None, **kwargs):\n",
    "        super(CustomPhoBERTLayer, self).__init__(**kwargs)\n",
    "        self.phobert = phobert_model\n",
    "        self.phobert_name = \"vinai/phobert-base\"  # Store the name for serialization\n",
    "        # Initialize the model if not provided\n",
    "        if self.phobert is None:\n",
    "            from transformers import TFAutoModel\n",
    "            self.phobert = TFAutoModel.from_pretrained(self.phobert_name)\n",
    "            self.phobert.trainable = False\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        output = self.phobert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomPhoBERTLayer, self).get_config()\n",
    "        config.update({\"phobert_name\": self.phobert_name})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Load PhoBERT when reconstructing the layer\n",
    "        config_copy = dict(config)\n",
    "        phobert_name = config_copy.pop(\"phobert_name\")\n",
    "        return cls(**config_copy)  # The __init__ will handle loading the model\n",
    "\n",
    "\n",
    "# üî• **Custom layer to extract CLS token safely**\n",
    "@register_keras_serializable()\n",
    "class CLSTokenExtractor(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CLSTokenExtractor, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Extract the [CLS] token (first token)\n",
    "        return inputs[:, 0, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CLSTokenExtractor, self).get_config()\n",
    "        return config\n",
    "\n",
    "\n",
    "# üî• **H√†m x√¢y d·ª±ng m√¥ h√¨nh PhoBERT**\n",
    "def build_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # ‚úÖ **D√πng l·ªõp CustomPhoBERTLayer thay v√¨ Lambda**\n",
    "    phobert_output = CustomPhoBERTLayer(phobert)([input_ids, attention_mask])\n",
    "\n",
    "    # üìå **L·∫•y embedding t·ª´ token ƒë·∫ßu ti√™n [CLS] - use custom layer instead of Lambda**\n",
    "    text_embedding = CLSTokenExtractor()(phobert_output)\n",
    "\n",
    "    dropout = tf.keras.layers.Dropout(0.1)(text_embedding)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# üöÄ **Load d·ªØ li·ªáu**\n",
    "real_news = pd.read_csv('./data/vnexpress_dataset.csv')\n",
    "fake_news = pd.read_csv('./data/vnexpress_fake_dataset.csv')\n",
    "\n",
    "# G√°n nh√£n\n",
    "real_news['Label'] = 0\n",
    "fake_news['Label'] = 1\n",
    "data = pd.concat([real_news, fake_news], ignore_index=True)\n",
    "\n",
    "# üöÄ **Chia th√†nh train (70%), validation (15%) v√† test (15%)**\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['Content'], data['Label'], test_size=0.3, random_state=42, stratify=data['Label']\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    test_texts, test_labels, test_size=0.5, random_state=42, stratify=test_labels\n",
    ")\n",
    "\n",
    "# üöÄ **Tokenize d·ªØ li·ªáu**\n",
    "train_inputs, train_mask, train_labels = preprocess_data(train_texts, train_labels)\n",
    "val_inputs, val_mask, val_labels = preprocess_data(val_texts, val_labels)\n",
    "test_inputs, test_mask, test_labels = preprocess_data(test_texts, test_labels)\n",
    "\n",
    "# üöÄ **T·∫°o dataset TensorFlow**\n",
    "# Smaller batch size for CPU\n",
    "batch_size = 8 if not gpus else 16\n",
    "\n",
    "# Optimize datasets for performance\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'input_ids': train_inputs, 'attention_mask': train_mask}, train_labels)) \\\n",
    "    .cache() \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_inputs, 'attention_mask': val_mask}, val_labels)) \\\n",
    "    .cache() \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'input_ids': test_inputs, 'attention_mask': test_mask}, test_labels)) \\\n",
    "    .cache() \\\n",
    "    .batch(batch_size) \\\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# üöÄ **Train model**\n",
    "model = build_model()\n",
    "\n",
    "# Create callback to monitor resource usage\n",
    "class ResourceMonitor(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print(f\"\\nüí• Resource usage before epoch {epoch+1}:\")\n",
    "        print(resource_usage())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\nüí• Resource usage after epoch {epoch+1}:\")\n",
    "        print(resource_usage())\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train with monitoring\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    callbacks=[ResourceMonitor()]\n",
    ")\n",
    "\n",
    "# Print training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nüí• Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "# üöÄ **ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p test**\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# üöÄ **D·ª± ƒëo√°n tr√™n t·∫≠p test**\n",
    "predictions = model.predict(test_dataset)\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "# Chuy·ªÉn ƒë·ªïi nh√£n v·ªÅ d·∫°ng 0 v√† 1\n",
    "predicted_labels = predicted_labels.flatten()\n",
    "test_labels = test_labels.numpy().flatten()\n",
    "# T·∫°o confusion matrix\n",
    "cm = confusion_matrix(test_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, predicted_labels, target_names=['Real', 'Fake']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84588fbea4315088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a555f384e4947dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:58:15.055867Z",
     "start_time": "2025-03-27T16:58:15.023034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "# üíæ L∆∞u m√¥ h√¨nh (Fixed model saving)\n",
    "try:\n",
    "    model.save('./model/fake_news_model.keras')\n",
    "    print(\"‚úÖ Model saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n",
    "    # Alternative saving method\n",
    "    print(\"Trying alternative save method...\")\n",
    "    model.save_weights('./model/fake_news_model.weights.h5')\n",
    "    print(\"‚úÖ Model weights saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c83a5d57c41c99e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T16:58:18.952790Z",
     "start_time": "2025-03-27T16:58:15.151115Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully\n",
      "\n",
      "üîç KI·ªÇM TRA TIN TH·∫¨T:\n",
      "N·ªôi dung: Th·ªß t∆∞·ªõng Ph·∫°m Minh Ch√≠nh cho bi·∫øt Vi·ªát Nam lu√¥n coi tr·ªçng h·ª£p t√°c v·ªõi EU v√† ƒë·ªÅ ngh·ªã EU s·ªõm ho√†n t·∫•t ph√™ chu·∫©n Hi·ªáp ƒë·ªãnh B·∫£o h·ªô ƒë·∫ßu t∆∞ Vi·ªát Nam-EU.\n",
      "K·∫øt qu·∫£: FAKE (ƒë·ªô tin c·∫≠y: 86.84%)\n",
      "\n",
      "üîç KI·ªÇM TRA TIN GI·∫¢:\n",
      "N·ªôi dung: Nh√† khoa h·ªçc Vi·ªát Nam ch·∫ø t·∫°o th√†nh c√¥ng m√°y ph√°t ƒëi·ªán vƒ©nh c·ª≠u kh√¥ng c·∫ßn nhi√™n li·ªáu, c√≥ th·ªÉ cung c·∫•p ƒëi·ªán mi·ªÖn ph√≠ cho to√†n b·ªô ƒë·∫•t n∆∞·ªõc.\n",
      "K·∫øt qu·∫£: FAKE (ƒë·ªô tin c·∫≠y: 89.43%)\n"
     ]
    }
   ],
   "source": [
    "# üìä T·∫°o h√†m ƒë·ªÉ d·ª± ƒëo√°n tin t·ª©c m·ªõi\n",
    "def predict_news(text, model, tokenizer):\n",
    "    # ƒê·∫£m b·∫£o text l√† string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Tokenize vƒÉn b·∫£n v·ªõi PhoBERT\n",
    "    inputs = tokenizer(text, padding='max_length', truncation=True, max_length=256, return_tensors='tf')\n",
    "\n",
    "    # D·ª± ƒëo√°n\n",
    "    prediction = model.predict({\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask']\n",
    "    }, verbose=0)\n",
    "\n",
    "    probability = prediction[0][0]\n",
    "\n",
    "    # Di·ªÖn gi·∫£i k·∫øt qu·∫£\n",
    "    result = \"FAKE\" if probability >= 0.5 else \"REAL\"\n",
    "    confidence = probability if probability >= 0.5 else 1 - probability\n",
    "\n",
    "    return {\n",
    "        'result': result,\n",
    "        'confidence': float(confidence),\n",
    "        'probability': float(probability)\n",
    "    }\n",
    "\n",
    "# üì± Load m√¥ h√¨nh ƒë√£ l∆∞u\n",
    "def load_model_for_inference():\n",
    "    try:\n",
    "        # No need to redefine the classes here since they're now properly registered\n",
    "        # Just load the model with custom_objects dictionary\n",
    "        loaded_model = tf.keras.models.load_model(\n",
    "            './model/fake_news_model.keras',\n",
    "            custom_objects={\n",
    "                'CustomPhoBERTLayer': CustomPhoBERTLayer,\n",
    "                'CLSTokenExtractor': CLSTokenExtractor\n",
    "            }\n",
    "        )\n",
    "        print(\"‚úÖ Model loaded successfully\")\n",
    "        return loaded_model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading full model: {e}\")\n",
    "        print(\"Loading model from architecture and weights...\")\n",
    "\n",
    "        # N·∫øu kh√¥ng th√†nh c√¥ng, t·∫°o l·∫°i model v√† load weights\n",
    "        new_model = build_model()\n",
    "        new_model.load_weights('./model/fake_news_model.weights.h5')\n",
    "        print(\"‚úÖ Model loaded from weights successfully\")\n",
    "        return new_model\n",
    "\n",
    "# üîç Test v·ªõi m·ªôt s·ªë v√≠ d·ª•\n",
    "try:\n",
    "    # Load model\n",
    "    inference_model = load_model_for_inference()\n",
    "\n",
    "    # V√≠ d·ª• v·ªÅ tin th·∫≠t\n",
    "    real_news_example = \"Th·ªß t∆∞·ªõng Ph·∫°m Minh Ch√≠nh cho bi·∫øt Vi·ªát Nam lu√¥n coi tr·ªçng h·ª£p t√°c v·ªõi EU v√† ƒë·ªÅ ngh·ªã EU s·ªõm ho√†n t·∫•t ph√™ chu·∫©n Hi·ªáp ƒë·ªãnh B·∫£o h·ªô ƒë·∫ßu t∆∞ Vi·ªát Nam-EU.\"\n",
    "\n",
    "    # V√≠ d·ª• v·ªÅ tin gi·∫£\n",
    "    fake_news_example = \"Nh√† khoa h·ªçc Vi·ªát Nam ch·∫ø t·∫°o th√†nh c√¥ng m√°y ph√°t ƒëi·ªán vƒ©nh c·ª≠u kh√¥ng c·∫ßn nhi√™n li·ªáu, c√≥ th·ªÉ cung c·∫•p ƒëi·ªán mi·ªÖn ph√≠ cho to√†n b·ªô ƒë·∫•t n∆∞·ªõc.\"\n",
    "\n",
    "    # D·ª± ƒëo√°n\n",
    "    real_result = predict_news(real_news_example, inference_model, tokenizer)\n",
    "    fake_result = predict_news(fake_news_example, inference_model, tokenizer)\n",
    "\n",
    "    # In k·∫øt qu·∫£\n",
    "    print(\"\\nüîç KI·ªÇM TRA TIN TH·∫¨T:\")\n",
    "    print(f\"N·ªôi dung: {real_news_example}\")\n",
    "    print(f\"K·∫øt qu·∫£: {real_result['result']} (ƒë·ªô tin c·∫≠y: {real_result['confidence']:.2%})\")\n",
    "\n",
    "    print(\"\\nüîç KI·ªÇM TRA TIN GI·∫¢:\")\n",
    "    print(f\"N·ªôi dung: {fake_news_example}\")\n",
    "    print(f\"K·∫øt qu·∫£: {fake_result['result']} (ƒë·ªô tin c·∫≠y: {fake_result['confidence']:.2%})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during inference: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
