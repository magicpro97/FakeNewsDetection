{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-24T17:24:33.493806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ðŸš€ KÃ­ch hoáº¡t GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# ðŸš€ Báº­t Mixed Precision Ä‘á»ƒ tÄƒng tá»‘c\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# ðŸš€ KÃ­ch hoáº¡t XLA compiler Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# Load PhoBERT tokenizer vÃ  model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert = TFAutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert.trainable = False  # âš¡ ÄÃ³ng bÄƒng PhoBERT\n",
    "\n",
    "# Preprocess dá»¯ liá»‡u\n",
    "def preprocess_data(data):\n",
    "    texts = data['Content'].tolist()\n",
    "    labels = data['Label'].tolist()\n",
    "\n",
    "    # Tokenize text vá»›i PhoBERT\n",
    "    inputs = tokenizer(texts, padding='max_length', truncation=True, max_length=256, return_tensors='tf')\n",
    "\n",
    "    return tf.convert_to_tensor(inputs['input_ids'], dtype=tf.int32), \\\n",
    "        tf.convert_to_tensor(inputs['attention_mask'], dtype=tf.int32), \\\n",
    "        tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Define mÃ´ hÃ¬nh classification dá»±a trÃªn PhoBERT\n",
    "def build_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # Encoder output\n",
    "    phobert_output = phobert(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "    # âš¡ Tá»‘i Æ°u: Láº¥y embedding tá»« token Ä‘áº§u tiÃªn [CLS] thay vÃ¬ trung bÃ¬nh toÃ n bá»™ hidden state\n",
    "    text_embedding = phobert_output[:, 0, :]\n",
    "\n",
    "    dropout = tf.keras.layers.Dropout(0.2)(text_embedding)  # âš¡ TÄƒng dropout Ä‘á»ƒ trÃ¡nh overfitting\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid', dtype=tf.float32)(dropout)  # âš¡ Mixed Precision fix\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load data\n",
    "real_news = pd.read_csv('./data/vnexpress_dataset.csv')\n",
    "fake_news = pd.read_csv('./data/vnexpress_fake_dataset.csv')\n",
    "\n",
    "# GÃ¡n nhÃ£n\n",
    "real_news['Label'] = 0\n",
    "fake_news['Label'] = 1\n",
    "data = pd.concat([real_news, fake_news], ignore_index=True)\n",
    "\n",
    "# Chia thÃ nh train (70%), validation (15%) vÃ  test (15%)\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['Content'], data['Label'], test_size=0.3, random_state=42, stratify=data['Label']\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    test_texts, test_labels, test_size=0.5, random_state=42, stratify=test_labels\n",
    ")\n",
    "\n",
    "# Tokenize dá»¯ liá»‡u\n",
    "train_inputs, train_mask, train_labels = preprocess_data(pd.DataFrame({'Content': train_texts, 'Label': train_labels}))\n",
    "val_inputs, val_mask, val_labels = preprocess_data(pd.DataFrame({'Content': val_texts, 'Label': val_labels}))\n",
    "test_inputs, test_mask, test_labels = preprocess_data(pd.DataFrame({'Content': test_texts, 'Label': test_labels}))\n",
    "\n",
    "# âš¡ TrÃ¡nh lá»—i dtype khÃ´ng khá»›p báº±ng cÃ¡ch chuyá»ƒn sang `int32`\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'input_ids': train_inputs, 'attention_mask': train_mask}, train_labels)) \\\n",
    "    .batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_inputs, 'attention_mask': val_mask}, val_labels)) \\\n",
    "    .batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'input_ids': test_inputs, 'attention_mask': test_mask}, test_labels)) \\\n",
    "    .batch(16).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Train model\n",
    "model = build_model()\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n",
    "\n",
    "# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ],
   "id": "9c596de0828959ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "The dtype policy mixed_float16 may run slowly because this machine does not have a GPU. Only Nvidia GPUs with compute capability of at least 7.0 run quickly with mixed_float16.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_4/roberta/pooler/dense/kernel:0', 'tf_roberta_model_4/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_4/roberta/pooler/dense/kernel:0', 'tf_roberta_model_4/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      " 14/146 [=>............................] - ETA: 28:30 - loss: 0.7970 - accuracy: 0.4464"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
