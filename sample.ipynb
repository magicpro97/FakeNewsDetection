{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-26T05:05:07.275379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ğŸš€ **KÃ­ch hoáº¡t GPU**\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# ğŸš€ **Báº­t Mixed Precision Ä‘á»ƒ tÄƒng tá»‘c**\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# ğŸš€ **KÃ­ch hoáº¡t XLA compiler Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t**\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "# ğŸ”¥ **Load PhoBERT tokenizer vÃ  model**\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert = TFAutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "phobert.trainable = False  # âš¡ ÄÃ³ng bÄƒng PhoBERT\n",
    "\n",
    "# ğŸš€ **HÃ m tiá»n xá»­ lÃ½ dá»¯ liá»‡u**\n",
    "def preprocess_data(data):\n",
    "    texts = data['Content'].tolist()\n",
    "    labels = data['Label'].tolist()\n",
    "\n",
    "    # Tokenize vÄƒn báº£n vá»›i PhoBERT\n",
    "    inputs = tokenizer(texts, padding='max_length', truncation=True, max_length=256, return_tensors='tf')\n",
    "\n",
    "    return tf.convert_to_tensor(inputs['input_ids'], dtype=tf.int32), \\\n",
    "           tf.convert_to_tensor(inputs['attention_mask'], dtype=tf.int32), \\\n",
    "           tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# ğŸ”¥ **Táº¡o lá»›p Keras tÃ¹y chá»‰nh cho PhoBERT**\n",
    "class CustomPhoBERTLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, phobert_model, **kwargs):\n",
    "        super(CustomPhoBERTLayer, self).__init__(**kwargs)\n",
    "        self.phobert = phobert_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        output = self.phobert(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        return output\n",
    "\n",
    "\n",
    "# ğŸ”¥ **HÃ m xÃ¢y dá»±ng mÃ´ hÃ¬nh PhoBERT**\n",
    "def build_model():\n",
    "    input_ids = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(256,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # âœ… **DÃ¹ng lá»›p CustomPhoBERTLayer thay vÃ¬ Lambda**\n",
    "    phobert_output = CustomPhoBERTLayer(phobert)([input_ids, attention_mask])\n",
    "\n",
    "    # ğŸ“Œ **Láº¥y embedding tá»« token Ä‘áº§u tiÃªn [CLS]**\n",
    "    text_embedding = tf.keras.layers.Lambda(lambda x: x[:, 0, :])(phobert_output)\n",
    "\n",
    "    dropout = tf.keras.layers.Dropout(0.1)(text_embedding)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(dropout)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ğŸš€ **Load dá»¯ liá»‡u**\n",
    "real_news = pd.read_csv('./data/vnexpress_dataset.csv')\n",
    "fake_news = pd.read_csv('./data/vnexpress_fake_dataset.csv')\n",
    "\n",
    "# GÃ¡n nhÃ£n\n",
    "real_news['Label'] = 0\n",
    "fake_news['Label'] = 1\n",
    "data = pd.concat([real_news, fake_news], ignore_index=True)\n",
    "\n",
    "# ğŸš€ **Chia thÃ nh train (70%), validation (15%) vÃ  test (15%)**\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    data['Content'], data['Label'], test_size=0.3, random_state=42, stratify=data['Label']\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    test_texts, test_labels, test_size=0.5, random_state=42, stratify=test_labels\n",
    ")\n",
    "\n",
    "# ğŸš€ **Tokenize dá»¯ liá»‡u**\n",
    "train_inputs, train_mask, train_labels = preprocess_data(pd.DataFrame({'Content': train_texts, 'Label': train_labels}))\n",
    "val_inputs, val_mask, val_labels = preprocess_data(pd.DataFrame({'Content': val_texts, 'Label': val_labels}))\n",
    "test_inputs, test_mask, test_labels = preprocess_data(pd.DataFrame({'Content': test_texts, 'Label': test_labels}))\n",
    "\n",
    "# ğŸš€ **Táº¡o dataset TensorFlow**\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'input_ids': train_inputs, 'attention_mask': train_mask}, train_labels)) \\\n",
    "    .batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': val_inputs, 'attention_mask': val_mask}, val_labels)) \\\n",
    "    .batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    ({'input_ids': test_inputs, 'attention_mask': test_mask}, test_labels)) \\\n",
    "    .batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ğŸš€ **Train model**\n",
    "model = build_model()\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=3)\n",
    "\n",
    "# ğŸš€ **ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn táº­p test**\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ],
   "id": "9c596de0828959ac",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001B[1m  1/146\u001B[0m \u001B[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[1m18:24\u001B[0m 8s/step - accuracy: 0.5625 - loss: 0.6953"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
