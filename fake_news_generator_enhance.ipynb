{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-20T02:52:00.308115Z"
    }
   },
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "import random\n",
    "import difflib\n",
    "import os\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "# Đọc dữ liệu từ vnexpress_dataset.csv\n",
    "df = pd.read_csv(\"./data/vnexpress_dataset.csv\")\n",
    "\n",
    "# Lọc tin trùng lặp từ bộ tin thật dựa trên Link\n",
    "initial_count = len(df)\n",
    "df = df.drop_duplicates(subset=[\"Link\"], keep=\"first\")\n",
    "print(f\"Đã loại bỏ {initial_count - len(df)} tin thật trùng lặp dựa trên Link. Còn lại {len(df)} tin thật.\")\n",
    "\n",
    "# (Tùy chọn) Lọc thêm trùng lặp dựa trên Content nếu cần\n",
    "# def is_content_too_similar(content1, content2, threshold=0.95):\n",
    "#     return difflib.SequenceMatcher(None, content1, content2).ratio() > threshold\n",
    "#\n",
    "# unique_contents = []\n",
    "# unique_indices = []\n",
    "# for idx, row in df.iterrows():\n",
    "#     content = row[\"Content\"]\n",
    "#     if not any(is_content_too_similar(content, existing_content) for existing_content in unique_contents):\n",
    "#         unique_contents.append(content)\n",
    "#         unique_indices.append(idx)\n",
    "# df = df.loc[unique_indices]\n",
    "# print(f\"Đã loại bỏ thêm {initial_count - len(unique_indices)} tin thật trùng lặp dựa trên Content. Còn lại {len(df)} tin thật.\")\n",
    "\n",
    "# Kiểm tra file tin giả đã tồn tại và lấy danh sách Link đã xử lý\n",
    "processed_links = set()\n",
    "fake_dataset_path = \"./data/vnexpress_fake_dataset_enhance.csv\"\n",
    "if os.path.exists(fake_dataset_path):\n",
    "    fake_df = pd.read_csv(fake_dataset_path)\n",
    "    processed_links = set(fake_df[\"Link\"].unique())\n",
    "    print(f\"Đã tìm thấy {len(processed_links)} Link đã được xử lý trong {fake_dataset_path}\")\n",
    "\n",
    "# Lọc các tin thật chưa được xử lý\n",
    "df = df[~df[\"Link\"].isin(processed_links)]\n",
    "print(f\"Còn lại {len(df)} tin thật chưa được xử lý để tạo tin giả\")\n",
    "\n",
    "# Hàm kiểm tra độ tương đồng giữa các nội dung\n",
    "def is_too_similar(content1, content2, threshold=0.9):\n",
    "    return difflib.SequenceMatcher(None, content1, content2).ratio() > threshold\n",
    "\n",
    "# Hàm gọi API để tạo tin giả cho một batch tin thật\n",
    "def generate_fake_news_batch(rows):\n",
    "    try:\n",
    "        # Chuẩn bị prompt cho batch\n",
    "        prompt = \"\"\n",
    "        for idx, row in enumerate(rows):\n",
    "            content = row[\"Content\"]\n",
    "            prompt += f\"\"\"\n",
    "            Tin thật {idx + 1}:\n",
    "            [TIN THẬT]: {content}\n",
    "\n",
    "            Hãy tạo 7 bản tin giả cho tin thật này. Mỗi bản tin giả phải:\n",
    "            - Có nội dung sai sự thật nhưng hợp lý, đủ thuyết phục để có thể bị nhầm là thật.\n",
    "            - Không sao chép nguyên văn tin thật, mà biến đổi câu chữ, thêm thông tin gây hiểu lầm hoặc bịa đặt.\n",
    "            - Có văn phong giống báo lá cải (giật gân, hấp dẫn) hoặc bài đăng mạng xã hội (ngắn gọn, kích thích tương tác, sử dụng từ ngữ lan truyền).\n",
    "            - Khác biệt rõ ràng so với tin thật và giữa các tin giả với nhau.\n",
    "            - Không sử dụng các từ ngữ hoặc chi tiết vi phạm chính sách nội dung (bạo lực, phân biệt đối xử, v.v.).\n",
    "\n",
    "            Định dạng đầu ra cho tin thật {idx + 1}:\n",
    "            - Mỗi bản tin giả là một đoạn văn ngắn (2-4 câu), không đánh số, không sử dụng tiêu đề phụ.\n",
    "            - Các đoạn cách nhau bằng một dòng trống.\n",
    "            - Bắt đầu với dòng: \"[Tin thật {idx + 1}]\", theo sau là các bản tin giả.\n",
    "\n",
    "            \"\"\"\n",
    "        prompt += \"\\nĐảm bảo các tin giả từ các tin thật khác nhau không trùng lặp nội dung.\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.8,\n",
    "        )\n",
    "\n",
    "        fake_news = []\n",
    "        current_tin_that_idx = None\n",
    "\n",
    "        for choice in response.choices:\n",
    "            # Tách nội dung theo các tin thật\n",
    "            for line in choice.message.content.split(\"\\n\"):\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"[Tin thật\"):\n",
    "                    current_tin_that_idx = int(line.split(\" \")[2].strip(\"]\")) - 1\n",
    "                    continue\n",
    "                if line and current_tin_that_idx is not None:\n",
    "                    row = rows[current_tin_that_idx]\n",
    "                    content = row[\"Content\"]\n",
    "                    # Kiểm tra không trùng với tin thật\n",
    "                    if not is_too_similar(line, content):\n",
    "                        fake_news.append({\n",
    "                            \"Title\": row[\"Title\"],\n",
    "                            \"Link\": row[\"Link\"],\n",
    "                            \"Views\": row[\"Views\"],\n",
    "                            \"Comments\": row[\"Comments\"],\n",
    "                            \"Content\": line,\n",
    "                            \"Label\": 1\n",
    "                        })\n",
    "\n",
    "        # Kiểm tra và loại bỏ tin giả trùng lặp\n",
    "        unique_fake_news = []\n",
    "        for news in fake_news:\n",
    "            is_unique = True\n",
    "            for existing_news in unique_fake_news:\n",
    "                if is_too_similar(news[\"Content\"], existing_news[\"Content\"]):\n",
    "                    is_unique = False\n",
    "                    break\n",
    "            if is_unique:\n",
    "                unique_fake_news.append(news)\n",
    "\n",
    "        return unique_fake_news\n",
    "    except Exception as err:\n",
    "        print(f\"Error processing batch: {err}\")\n",
    "        return []\n",
    "\n",
    "# Số lượng worker và batch size\n",
    "MAX_WORKERS = 5\n",
    "BATCH_SIZE = 3  # Số tin thật mỗi lần gửi API\n",
    "\n",
    "fake_news_list = []\n",
    "\n",
    "# Chỉ xử lý nếu còn tin thật chưa được tạo tin giả\n",
    "if len(df) > 0:\n",
    "    # Chia dữ liệu thành các batch\n",
    "    batches = [df.iloc[i:i + BATCH_SIZE].to_dict('records') for i in range(0, len(df), BATCH_SIZE)]\n",
    "\n",
    "    # Xử lý song song với ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Tạo dictionary của các future với tham số đầu vào\n",
    "        future_to_batch = {executor.submit(generate_fake_news_batch, batch): i for i, batch in enumerate(batches)}\n",
    "\n",
    "        # Xử lý kết quả khi hoàn thành\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_batch), total=len(future_to_batch), desc=\"Generating fake news\"):\n",
    "            try:\n",
    "                results = future.result()\n",
    "                fake_news_list.extend(results)\n",
    "                # Giảm tải API bằng cách đợi một chút giữa các yêu cầu\n",
    "                time.sleep(random.uniform(0.3, 0.7))\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred: {e}\")\n",
    "\n",
    "    # Lưu tin giả vào file (nối thêm vào file hiện có)\n",
    "    if fake_news_list:\n",
    "        fake_df = pd.DataFrame(fake_news_list)\n",
    "        if os.path.exists(fake_dataset_path):\n",
    "            # Nối thêm vào file hiện có\n",
    "            fake_df.to_csv(fake_dataset_path, mode='a', header=False, index=False, encoding=\"utf-8-sig\")\n",
    "        else:\n",
    "            # Tạo file mới\n",
    "            fake_df.to_csv(fake_dataset_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"✅ Đã tạo và lưu {len(fake_news_list)} tin giả mới vào {fake_dataset_path}\")\n",
    "else:\n",
    "    print(\"⚠️ Không có tin thật mới nào để tạo tin giả\")\n",
    "\n",
    "# Xử lý mất cân bằng: Chọn ngẫu nhiên 2 tin giả từ mỗi tin thật\n",
    "balanced_fake_news_list = []\n",
    "fake_df = pd.read_csv(fake_dataset_path)  # Đọc lại file để lấy tất cả tin giả\n",
    "for link in fake_df[\"Link\"].unique():\n",
    "    fake_subset = fake_df[fake_df[\"Link\"] == link].to_dict('records')\n",
    "    # Chọn ngẫu nhiên 2 tin giả từ mỗi Link\n",
    "    selected_fakes = random.sample(fake_subset, min(2, len(fake_subset)))\n",
    "    balanced_fake_news_list.extend(selected_fakes)\n",
    "\n",
    "# Tạo tập dữ liệu tin thật (sử dụng bộ tin thật đã lọc trùng lặp)\n",
    "real_news_list = [\n",
    "    {\n",
    "        \"Title\": row[\"Title\"],\n",
    "        \"Link\": row[\"Link\"],\n",
    "        \"Views\": row[\"Views\"],\n",
    "        \"Comments\": row[\"Comments\"],\n",
    "        \"Content\": row[\"Content\"],\n",
    "        \"Label\": 0\n",
    "    } for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Kết hợp tin thật và tin giả\n",
    "combined_news_list = real_news_list + balanced_fake_news_list\n",
    "combined_df = pd.DataFrame(combined_news_list)\n",
    "\n",
    "# Lưu vào file kết hợp\n",
    "combined_df.to_csv(\"./data/vnexpress_combined_dataset.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Đã tạo và lưu {len(combined_news_list)} bản tin (thật + giả) thành công vào vnexpress_combined_dataset.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã loại bỏ 62 tin thật trùng lặp dựa trên Link. Còn lại 6473 tin thật.\n",
      "Đã tìm thấy 1610 Link đã được xử lý trong ./data/vnexpress_fake_dataset_enhance.csv\n",
      "Còn lại 4863 tin thật chưa được xử lý để tạo tin giả\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating fake news:   6%|▋         | 104/1621 [05:20<1:38:56,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating fake news:  10%|█         | 170/1621 [08:30<45:05,  1.86s/it]  "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "29c19e27abbd0179"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
