{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-20T15:35:40.855001Z",
     "start_time": "2025-04-20T15:35:37.866174Z"
    }
   },
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "import random\n",
    "import difflib\n",
    "import os\n",
    "import csv\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=\"\"\n",
    ")\n",
    "\n",
    "# Đọc dữ liệu từ vnexpress_dataset.csv\n",
    "df = pd.read_csv(\"./data/vnexpress_dataset.csv\", quoting=csv.QUOTE_MINIMAL, escapechar='\\\\')\n",
    "\n",
    "# Lọc tin trùng lặp từ bộ tin thật dựa trên Link\n",
    "initial_count = len(df)\n",
    "df = df.drop_duplicates(subset=[\"Link\"], keep=\"first\")\n",
    "print(f\"Đã loại bỏ {initial_count - len(df)} tin thật trùng lặp dựa trên Link. Còn lại {len(df)} tin thật.\")\n",
    "\n",
    "# Kiểm tra file combined dataset đã tồn tại\n",
    "combined_dataset_path = \"./data/vnexpress_combined_dataset.csv\"\n",
    "combined_links_real = set()\n",
    "combined_links_fake = set()\n",
    "if os.path.exists(combined_dataset_path):\n",
    "    try:\n",
    "        combined_df = pd.read_csv(combined_dataset_path, quoting=csv.QUOTE_MINIMAL, escapechar='\\\\', on_bad_lines='skip')\n",
    "        # Đảm bảo có cột Label trong combined dataset\n",
    "        if 'Label' not in combined_df.columns:\n",
    "            combined_df['Label'] = 0  # Mặc định là tin thật\n",
    "            print(f\"Đã thêm cột Label vào file {combined_dataset_path}\")\n",
    "            # Lưu lại file với cột Label\n",
    "            combined_df.to_csv(combined_dataset_path, index=False, encoding=\"utf-8-sig\", quoting=csv.QUOTE_MINIMAL, escapechar='\\\\')\n",
    "\n",
    "        # Lấy danh sách Link đã có trong combined dataset\n",
    "        combined_links_real = set(combined_df[combined_df['Label'] == 0]['Link'].unique())\n",
    "        combined_links_fake = set(combined_df[combined_df['Label'] == 1]['Link'].unique())\n",
    "        print(f\"Đã tìm thấy {len(combined_links_real)} Link tin thật và {len(combined_links_fake)} Link tin giả trong {combined_dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file {combined_dataset_path}: {e}\")\n",
    "        combined_links_real = set()\n",
    "        combined_links_fake = set()\n",
    "\n",
    "# (Tùy chọn) Lọc thêm trùng lặp dựa trên Content nếu cần\n",
    "# def is_content_too_similar(content1, content2, threshold=0.95):\n",
    "#     return difflib.SequenceMatcher(None, content1, content2).ratio() > threshold\n",
    "#\n",
    "# unique_contents = []\n",
    "# unique_indices = []\n",
    "# for idx, row in df.iterrows():\n",
    "#     content = row[\"Content\"]\n",
    "#     if not any(is_content_too_similar(content, existing_content) for existing_content in unique_contents):\n",
    "#         unique_contents.append(content)\n",
    "#         unique_indices.append(idx)\n",
    "# df = df.loc[unique_indices]\n",
    "# print(f\"Đã loại bỏ thêm {initial_count - len(unique_indices)} tin thật trùng lặp dựa trên Content. Còn lại {len(df)} tin thật.\")\n",
    "\n",
    "# Kiểm tra file tin giả đã tồn tại và lấy danh sách Link đã xử lý\n",
    "processed_links = set()\n",
    "fake_dataset_path = \"./data/vnexpress_fake_dataset_enhance.csv\"\n",
    "if os.path.exists(fake_dataset_path):\n",
    "    fake_df = pd.read_csv(fake_dataset_path, quoting=csv.QUOTE_MINIMAL, escapechar='\\\\', on_bad_lines='skip')\n",
    "    processed_links = set(fake_df[\"Link\"].unique())\n",
    "    print(f\"Đã tìm thấy {len(processed_links)} Link đã được xử lý trong {fake_dataset_path}\")\n",
    "\n",
    "# Lọc các tin thật chưa được xử lý và chưa có trong combined dataset\n",
    "df = df[~df[\"Link\"].isin(processed_links) & ~df[\"Link\"].isin(combined_links_real)]\n",
    "print(f\"Còn lại {len(df)} tin thật chưa được xử lý để tạo tin giả\")\n",
    "\n",
    "# Hàm kiểm tra độ tương đồng giữa các nội dung\n",
    "def is_too_similar(content1, content2, threshold=0.9):\n",
    "    return difflib.SequenceMatcher(None, content1, content2).ratio() > threshold\n",
    "\n",
    "# Hàm gọi API để tạo tin giả cho một batch tin thật\n",
    "def generate_fake_news_batch(rows):\n",
    "    try:\n",
    "        # Chuẩn bị prompt cho batch\n",
    "        prompt = \"\"\n",
    "        for idx, row in enumerate(rows):\n",
    "            content = row[\"Content\"]\n",
    "            prompt += f\"\"\"\n",
    "            Tin thật {idx + 1}:\n",
    "            [TIN THẬT]: {content}\n",
    "\n",
    "            Hãy tạo 7 bản tin giả cho tin thật này. Mỗi bản tin giả phải:\n",
    "            - Có nội dung sai sự thật nhưng hợp lý, đủ thuyết phục để có thể bị nhầm là thật.\n",
    "            - Không sao chép nguyên văn tin thật, mà biến đổi câu chữ, thêm thông tin gây hiểu lầm hoặc bịa đặt.\n",
    "            - Có văn phong giống báo lá cải (giật gân, hấp dẫn) hoặc bài đăng mạng xã hội (ngắn gọn, kích thích tương tác, sử dụng từ ngữ lan truyền).\n",
    "            - Khác biệt rõ ràng so với tin thật và giữa các tin giả với nhau.\n",
    "            - Không sử dụng các từ ngữ hoặc chi tiết vi phạm chính sách nội dung (bạo lực, phân biệt đối xử, v.v.).\n",
    "\n",
    "            Định dạng đầu ra cho tin thật {idx + 1}:\n",
    "            - Mỗi bản tin giả là một đoạn văn ngắn (2-4 câu), không đánh số, không sử dụng tiêu đề phụ.\n",
    "            - Các đoạn cách nhau bằng một dòng trống.\n",
    "            - Bắt đầu với dòng: \"[Tin thật {idx + 1}]\", theo sau là các bản tin giả.\n",
    "\n",
    "            \"\"\"\n",
    "        prompt += \"\\nĐảm bảo các tin giả từ các tin thật khác nhau không trùng lặp nội dung.\"\n",
    "\n",
    "        # Thêm cơ chế retry với exponential backoff\n",
    "        max_retries = 3\n",
    "        retry_delay = 1  # Thời gian chờ ban đầu (giây)\n",
    "        response = []\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o-mini\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.8,\n",
    "                )\n",
    "                break  # Nếu thành công, thoát khỏi vòng lặp\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:  # Nếu chưa phải lần thử cuối cùng\n",
    "                    print(f\"Lỗi kết nối lần {attempt+1}: {e}. Thử lại sau {retry_delay} giây...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Tăng thời gian chờ theo cấp số nhân\n",
    "                else:\n",
    "                    # Nếu đã thử hết số lần cho phép, ném lại ngoại lệ\n",
    "                    raise e\n",
    "\n",
    "        fake_news = []\n",
    "        current_tin_that_idx = None\n",
    "\n",
    "        for choice in response.choices:\n",
    "            # Tách nội dung theo các tin thật\n",
    "            for line in choice.message.content.split(\"\\n\"):\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"[Tin thật\"):\n",
    "                    try:\n",
    "                        parts = line.split(\" \")\n",
    "                        if len(parts) > 2:\n",
    "                            current_tin_that_idx = int(parts[2].strip(\"]\")) - 1\n",
    "                        else:\n",
    "                            # If format is unexpected, skip this line\n",
    "                            continue\n",
    "                    except (ValueError, IndexError):\n",
    "                        # If parsing fails, skip this line\n",
    "                        continue\n",
    "                    continue\n",
    "                if line and current_tin_that_idx is not None:\n",
    "                    # Check if current_tin_that_idx is valid for rows\n",
    "                    if 0 <= current_tin_that_idx < len(rows):\n",
    "                        row = rows[current_tin_that_idx]\n",
    "                        content = row[\"Content\"]\n",
    "                        # Kiểm tra không trùng với tin thật\n",
    "                        if not is_too_similar(line, content):\n",
    "                            fake_news.append({\n",
    "                                \"Title\": row[\"Title\"],\n",
    "                                \"Link\": row[\"Link\"],\n",
    "                                \"Views\": row[\"Views\"],\n",
    "                                \"Comments\": row[\"Comments\"],\n",
    "                                \"Content\": line,\n",
    "                                \"Label\": 1\n",
    "                            })\n",
    "                    else:\n",
    "                        # Skip if index is out of range\n",
    "                        continue\n",
    "\n",
    "        # Kiểm tra và loại bỏ tin giả trùng lặp\n",
    "        unique_fake_news = []\n",
    "        for news in fake_news:\n",
    "            is_unique = True\n",
    "            for existing_news in unique_fake_news:\n",
    "                if is_too_similar(news[\"Content\"], existing_news[\"Content\"]):\n",
    "                    is_unique = False\n",
    "                    break\n",
    "            if is_unique:\n",
    "                unique_fake_news.append(news)\n",
    "\n",
    "        return unique_fake_news\n",
    "    except Exception as err:\n",
    "        print(f\"Error processing batch: {err}\")\n",
    "        return []\n",
    "\n",
    "# Số lượng worker và batch size\n",
    "MAX_WORKERS = 5\n",
    "BATCH_SIZE = 3  # Số tin thật mỗi lần gửi API\n",
    "\n",
    "fake_news_list = []\n",
    "\n",
    "# Chỉ xử lý nếu còn tin thật chưa được tạo tin giả\n",
    "if len(df) > 0:\n",
    "    # Chia dữ liệu thành các batch\n",
    "    batches = [df.iloc[i:i + BATCH_SIZE].to_dict('records') for i in range(0, len(df), BATCH_SIZE)]\n",
    "\n",
    "    # Xử lý song song với ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Tạo dictionary của các future với tham số đầu vào\n",
    "        future_to_batch = {executor.submit(generate_fake_news_batch, batch): i for i, batch in enumerate(batches)}\n",
    "\n",
    "        # Xử lý kết quả khi hoàn thành\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_batch), total=len(future_to_batch), desc=\"Generating fake news\"):\n",
    "            try:\n",
    "                results = future.result()\n",
    "                # Check if results is iterable before extending\n",
    "                if isinstance(results, (list, tuple)) and results:\n",
    "                    fake_news_list.extend(results)\n",
    "                elif results and not isinstance(results, (list, tuple)):\n",
    "                    print(f\"Warning: Expected iterable but got {type(results)}\")\n",
    "                # Giảm tải API bằng cách đợi một chút giữa các yêu cầu\n",
    "                time.sleep(random.uniform(0.3, 0.7))\n",
    "            except Exception as e:\n",
    "                print(f\"Exception occurred: {e}\")\n",
    "\n",
    "    # Lưu tin giả vào file (nối thêm vào file hiện có)\n",
    "    if fake_news_list:\n",
    "        fake_df = pd.DataFrame(fake_news_list)\n",
    "        # Đảm bảo Label=1 cho tất cả tin giả\n",
    "        if 'Label' not in fake_df.columns:\n",
    "            fake_df['Label'] = 1\n",
    "        if os.path.exists(fake_dataset_path):\n",
    "            # Nối thêm vào file hiện có\n",
    "            fake_df.to_csv(fake_dataset_path, mode='a', header=False, index=False, encoding=\"utf-8-sig\", quoting=csv.QUOTE_MINIMAL, escapechar='\\\\')\n",
    "        else:\n",
    "            # Tạo file mới\n",
    "            fake_df.to_csv(fake_dataset_path, index=False, encoding=\"utf-8-sig\", quoting=csv.QUOTE_MINIMAL, escapechar='\\\\')\n",
    "        print(f\"✅ Đã tạo và lưu {len(fake_news_list)} tin giả mới vào {fake_dataset_path}\")\n",
    "else:\n",
    "    print(\"⚠️ Không có tin thật mới nào để tạo tin giả\")\n",
    "\n",
    "# Xử lý mất cân bằng: Chọn ngẫu nhiên 2 tin giả từ mỗi tin thật\n",
    "balanced_fake_news_list = []\n",
    "fake_df = pd.read_csv(fake_dataset_path, quoting=csv.QUOTE_MINIMAL, escapechar='\\\\', on_bad_lines='skip')  # Đọc lại file để lấy tất cả tin giả\n",
    "# Đảm bảo có cột Label và giá trị là 1 cho tất cả tin giả\n",
    "if 'Label' not in fake_df.columns:\n",
    "    fake_df['Label'] = 1\n",
    "for link in fake_df[\"Link\"].unique():\n",
    "    # Bỏ qua các Link đã có tin giả trong combined dataset\n",
    "    if link in combined_links_fake:\n",
    "        continue\n",
    "\n",
    "    fake_subset = fake_df[fake_df[\"Link\"] == link].to_dict('records')\n",
    "    # Chọn ngẫu nhiên 2 tin giả từ mỗi Link nếu có ít nhất 1 tin giả\n",
    "    if fake_subset:  # Check if fake_subset is not empty\n",
    "        selected_fakes = random.sample(fake_subset, min(2, len(fake_subset)))\n",
    "        # Đảm bảo mỗi tin giả đều có Label=1\n",
    "        for fake in selected_fakes:\n",
    "            if 'Label' not in fake:\n",
    "                fake['Label'] = 1\n",
    "        balanced_fake_news_list.extend(selected_fakes)\n",
    "\n",
    "# Tạo tập dữ liệu tin thật (sử dụng bộ tin thật đã lọc trùng lặp)\n",
    "real_news_list = [\n",
    "    {\n",
    "        \"Title\": row[\"Title\"],\n",
    "        \"Link\": row[\"Link\"],\n",
    "        \"Views\": row[\"Views\"],\n",
    "        \"Comments\": row[\"Comments\"],\n",
    "        \"Content\": row[\"Content\"],\n",
    "        \"Label\": 0\n",
    "    } for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Kết hợp tin thật và tin giả mới\n",
    "combined_news_list = real_news_list + balanced_fake_news_list\n",
    "\n",
    "# Nếu file combined dataset đã tồn tại, đọc nó và thêm vào các bản ghi mới\n",
    "if os.path.exists(combined_dataset_path) and len(combined_news_list) > 0:\n",
    "    try:\n",
    "        existing_combined_df = pd.read_csv(combined_dataset_path, quoting=csv.QUOTE_MINIMAL, escapechar='\\\\', on_bad_lines='skip')\n",
    "        # Đảm bảo có cột Label\n",
    "        if 'Label' not in existing_combined_df.columns:\n",
    "            existing_combined_df['Label'] = 0  # Mặc định là tin thật\n",
    "\n",
    "        # Tạo DataFrame mới từ combined_news_list\n",
    "        new_combined_df = pd.DataFrame(combined_news_list)\n",
    "\n",
    "        # Nối DataFrame cũ và mới\n",
    "        combined_df = pd.concat([existing_combined_df, new_combined_df], ignore_index=True)\n",
    "\n",
    "        # Loại bỏ các bản ghi trùng lặp dựa trên Link và Content\n",
    "        combined_df = combined_df.drop_duplicates(subset=[\"Link\", \"Content\"], keep=\"first\")\n",
    "\n",
    "        print(f\"Đã thêm {len(new_combined_df)} bản ghi mới vào file {combined_dataset_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi đọc file {combined_dataset_path}: {e}\")\n",
    "        combined_df = pd.DataFrame(combined_news_list)\n",
    "else:\n",
    "    combined_df = pd.DataFrame(combined_news_list)\n",
    "\n",
    "# Lưu vào file kết hợp nếu có dữ liệu mới\n",
    "if len(combined_news_list) > 0:\n",
    "    combined_df.to_csv(combined_dataset_path, index=False, encoding=\"utf-8-sig\", quoting=csv.QUOTE_MINIMAL, escapechar='\\\\')\n",
    "\n",
    "if len(combined_news_list) > 0:\n",
    "    print(f\"✅ Đã tạo và lưu {len(combined_news_list)} bản tin (thật + giả) mới thành công vào vnexpress_combined_dataset.csv\")\n",
    "else:\n",
    "    print(\"⚠️ Không có bản tin mới nào được thêm vào vnexpress_combined_dataset.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã loại bỏ 62 tin thật trùng lặp dựa trên Link. Còn lại 6473 tin thật.\n",
      "Đã tìm thấy 4863 Link tin thật và 1314 Link tin giả trong ./data/vnexpress_combined_dataset.csv\n",
      "Đã tìm thấy 1610 Link đã được xử lý trong ./data/vnexpress_fake_dataset_enhance.csv\n",
      "Còn lại 0 tin thật chưa được xử lý để tạo tin giả\n",
      "⚠️ Không có tin thật mới nào để tạo tin giả\n",
      "Đã thêm 592 bản ghi mới vào file ./data/vnexpress_combined_dataset.csv\n",
      "✅ Đã tạo và lưu 592 bản tin (thật + giả) mới thành công vào vnexpress_combined_dataset.csv\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "29c19e27abbd0179"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
