{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Classifier on Google Colab\n",
    "\n",
    "This notebook implements a fake news classifier using PhoBERT and TensorFlow, adapted from the `fake_news_classifier_no_shadow.py` script. It is designed to run on Google Colab with GPU support.\n",
    "\n",
    "## Setup Instructions\n",
    "1. **Enable GPU**: Go to `Runtime > Change runtime type` and select `GPU`.\n",
    "2. Run the cells below to install dependencies and execute the code.\n",
    "3. **Upload Data**: Ensure `vnexpress_dataset.csv` and `vnexpress_fake_dataset.csv` are uploaded to the Colab environment (use the file upload feature in the left sidebar).\n",
    "4. Monitor logs and TensorBoard for training progress and results.\n",
    "\n",
    "**Note**: You may need to restart the runtime after installing dependencies if prompted."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T18:45:32.344660Z",
     "start_time": "2025-04-19T18:45:32.051102Z"
    }
   },
   "source": [
    "!conda install -y tensorflow=2.10 cudatoolkit=11.2 cudnn=8.1 -c conda-forge\n",
    "!conda install -y pandas=1.5 scikit-learn=1.2 imbalanced-learn=0.10 numpy=1.23 -c conda-forge\n",
    "!pip install transformers==4.25\n",
    "\n",
    "# Verify installations\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import imblearn\n",
    "import numpy as np\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"Imbalanced-learn version: {imblearn.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "\n",
    "# Check GPU availability and CUDA/CUDNN\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"CUDA Available:\", tf.test.is_built_with_cuda())\n",
    "try:\n",
    "    import subprocess\n",
    "    nvcc_output = subprocess.check_output(['nvcc', '--version']).decode('utf-8')\n",
    "    print(\"CUDA Version (nvcc):\", nvcc_output.split('\\n')[3])\n",
    "except Exception as err:\n",
    "    print(f\"CUDA Version (nvcc): Not found {err}\")\n",
    "\n",
    "# Check TensorBoard compatibility\n",
    "try:\n",
    "    from tensorboard import program\n",
    "    print(\"TensorBoard is available\")\n",
    "except ImportError:\n",
    "    print(\"TensorBoard is not available, please ensure TensorFlow is installed correctly\")"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Install required libraries\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#!conda install -y tensorflow-gpu cudatoolkit pandas scikit-learn\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m#!pip install transformers\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Verify installations\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTensorFlow version: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtf\u001B[38;5;241m.\u001B[39m__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/tf-gpu-cuda/lib/python3.9/site-packages/tensorflow/__init__.py:41\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msix\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m_six\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msys\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m module_util \u001B[38;5;28;01mas\u001B[39;00m _module_util\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy_loader\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LazyLoader \u001B[38;5;28;01mas\u001B[39;00m _LazyLoader\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/tf-gpu-cuda/lib/python3.9/site-packages/tensorflow/python/__init__.py:41\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# import it in modules_with_exports.py instead.\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# go/tf-wildcard-import\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m pywrap_tensorflow \u001B[38;5;28;01mas\u001B[39;00m _pywrap_tensorflow\n\u001B[0;32m---> 41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# pylint: enable=wildcard-import\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# Bring in subpackages.\u001B[39;00m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m data\n",
      "File \u001B[0;32m~/anaconda3/envs/tf-gpu-cuda/lib/python3.9/site-packages/tensorflow/python/eager/context.py:32\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msix\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m function_pb2\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m config_pb2\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotobuf\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m rewriter_config_pb2\n",
      "File \u001B[0;32m~/anaconda3/envs/tf-gpu-cuda/lib/python3.9/site-packages/tensorflow/core/framework/function_pb2.py:16\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# @@protoc_insertion_point(imports)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m _sym_db \u001B[38;5;241m=\u001B[39m _symbol_database\u001B[38;5;241m.\u001B[39mDefault()\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m attr_value_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m node_def_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m op_def_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_op__def__pb2\n",
      "File \u001B[0;32m~/anaconda3/envs/tf-gpu-cuda/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:16\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# @@protoc_insertion_point(imports)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m _sym_db \u001B[38;5;241m=\u001B[39m _symbol_database\u001B[38;5;241m.\u001B[39mDefault()\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tensor_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tensor_shape_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m types_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001B[0;32m~/anaconda3/envs/tf-gpu-cuda/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:16\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# @@protoc_insertion_point(imports)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m _sym_db \u001B[38;5;241m=\u001B[39m _symbol_database\u001B[38;5;241m.\u001B[39mDefault()\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m resource_handle_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tensor_shape_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m types_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001B[0;32m~/anaconda3/envs/tf-gpu-cuda/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:16\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# @@protoc_insertion_point(imports)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m _sym_db \u001B[38;5;241m=\u001B[39m _symbol_database\u001B[38;5;241m.\u001B[39mDefault()\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tensor_shape_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m types_pb2 \u001B[38;5;28;01mas\u001B[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n\u001B[1;32m     20\u001B[0m DESCRIPTOR \u001B[38;5;241m=\u001B[39m _descriptor\u001B[38;5;241m.\u001B[39mFileDescriptor(\n\u001B[1;32m     21\u001B[0m   name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensorflow/core/framework/resource_handle.proto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     22\u001B[0m   package\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensorflow\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     26\u001B[0m   ,\n\u001B[1;32m     27\u001B[0m   dependencies\u001B[38;5;241m=\u001B[39m[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001B[38;5;241m.\u001B[39mDESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2\u001B[38;5;241m.\u001B[39mDESCRIPTOR,])\n",
      "File \u001B[0;32m~/anaconda3/envs/tf-gpu-cuda/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36\u001B[0m\n\u001B[1;32m     13\u001B[0m _sym_db \u001B[38;5;241m=\u001B[39m _symbol_database\u001B[38;5;241m.\u001B[39mDefault()\n\u001B[1;32m     18\u001B[0m DESCRIPTOR \u001B[38;5;241m=\u001B[39m _descriptor\u001B[38;5;241m.\u001B[39mFileDescriptor(\n\u001B[1;32m     19\u001B[0m   name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensorflow/core/framework/tensor_shape.proto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     20\u001B[0m   package\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensorflow\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m   serialized_pb\u001B[38;5;241m=\u001B[39m_b(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m,tensorflow/core/framework/tensor_shape.proto\u001B[39m\u001B[38;5;130;01m\\x12\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mtensorflow\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;124mz\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\x10\u001B[39;00m\u001B[38;5;124mTensorShapeProto\u001B[39m\u001B[38;5;130;01m\\x12\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\x03\u001B[39;00m\u001B[38;5;130;01m\\x64\u001B[39;00m\u001B[38;5;124mim\u001B[39m\u001B[38;5;130;01m\\x18\u001B[39;00m\u001B[38;5;130;01m\\x02\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\x03\u001B[39;00m\u001B[38;5;124m(\u001B[39m\u001B[38;5;130;01m\\x0b\u001B[39;00m\u001B[38;5;130;01m\\x32\u001B[39;00m\u001B[38;5;124m .tensorflow.TensorShapeProto.Dim\u001B[39m\u001B[38;5;130;01m\\x12\u001B[39;00m\u001B[38;5;130;01m\\x14\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\x0c\u001B[39;00m\u001B[38;5;124munknown_rank\u001B[39m\u001B[38;5;130;01m\\x18\u001B[39;00m\u001B[38;5;130;01m\\x03\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\x01\u001B[39;00m\u001B[38;5;124m(\u001B[39m\u001B[38;5;130;01m\\x08\u001B[39;00m\u001B[38;5;130;01m\\x1a\u001B[39;00m\u001B[38;5;124m!\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\x03\u001B[39;00m\u001B[38;5;130;01m\\x44\u001B[39;00m\u001B[38;5;124mim\u001B[39m\u001B[38;5;130;01m\\x12\u001B[39;00m\u001B[38;5;130;01m\\x0c\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\x04\u001B[39;00m\u001B[38;5;124msize\u001B[39m\u001B[38;5;130;01m\\x18\u001B[39;00m\u001B[38;5;130;01m\\x01\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\x01\u001B[39;00m\u001B[38;5;124m(\u001B[39m\u001B[38;5;130;01m\\x03\u001B[39;00m\u001B[38;5;130;01m\\x12\u001B[39;00m\u001B[38;5;130;01m\\x0c\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\x04\u001B[39;00m\u001B[38;5;124mname\u001B[39m\u001B[38;5;130;01m\\x18\u001B[39;00m\u001B[38;5;130;01m\\x02\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;130;01m\\x01\u001B[39;00m\u001B[38;5;124m(\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mB\u001B[39m\u001B[38;5;130;01m\\x87\u001B[39;00m\u001B[38;5;130;01m\\x01\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\x18\u001B[39;00m\u001B[38;5;124morg.tensorflow.frameworkB\u001B[39m\u001B[38;5;130;01m\\x11\u001B[39;00m\u001B[38;5;124mTensorShapeProtosP\u001B[39m\u001B[38;5;130;01m\\x01\u001B[39;00m\u001B[38;5;124mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001B[39m\u001B[38;5;130;01m\\xf8\u001B[39;00m\u001B[38;5;130;01m\\x01\u001B[39;00m\u001B[38;5;130;01m\\x01\u001B[39;00m\u001B[38;5;130;01m\\x62\u001B[39;00m\u001B[38;5;130;01m\\x06\u001B[39;00m\u001B[38;5;124mproto3\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     24\u001B[0m )\n\u001B[1;32m     29\u001B[0m _TENSORSHAPEPROTO_DIM \u001B[38;5;241m=\u001B[39m _descriptor\u001B[38;5;241m.\u001B[39mDescriptor(\n\u001B[1;32m     30\u001B[0m   name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDim\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     31\u001B[0m   full_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensorflow.TensorShapeProto.Dim\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     32\u001B[0m   filename\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     33\u001B[0m   file\u001B[38;5;241m=\u001B[39mDESCRIPTOR,\n\u001B[1;32m     34\u001B[0m   containing_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     35\u001B[0m   fields\u001B[38;5;241m=\u001B[39m[\n\u001B[0;32m---> 36\u001B[0m     \u001B[43m_descriptor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFieldDescriptor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfull_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtensorflow.TensorShapeProto.Dim.size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     38\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnumber\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcpp_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m      \u001B[49m\u001B[43mhas_default_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmessage_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menum_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontaining_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m      \u001B[49m\u001B[43mis_extension\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextension_scope\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m      \u001B[49m\u001B[43mserialized_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDESCRIPTOR\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m     43\u001B[0m     _descriptor\u001B[38;5;241m.\u001B[39mFieldDescriptor(\n\u001B[1;32m     44\u001B[0m       name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m, full_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensorflow.TensorShapeProto.Dim.name\u001B[39m\u001B[38;5;124m'\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     45\u001B[0m       number\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;28mtype\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m9\u001B[39m, cpp_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m9\u001B[39m, label\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m     46\u001B[0m       has_default_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, default_value\u001B[38;5;241m=\u001B[39m_b(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[1;32m     47\u001B[0m       message_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, enum_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, containing_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     48\u001B[0m       is_extension\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, extension_scope\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     49\u001B[0m       serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, file\u001B[38;5;241m=\u001B[39mDESCRIPTOR),\n\u001B[1;32m     50\u001B[0m   ],\n\u001B[1;32m     51\u001B[0m   extensions\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m     52\u001B[0m   ],\n\u001B[1;32m     53\u001B[0m   nested_types\u001B[38;5;241m=\u001B[39m[],\n\u001B[1;32m     54\u001B[0m   enum_types\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m     55\u001B[0m   ],\n\u001B[1;32m     56\u001B[0m   serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     57\u001B[0m   is_extendable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     58\u001B[0m   syntax\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mproto3\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     59\u001B[0m   extension_ranges\u001B[38;5;241m=\u001B[39m[],\n\u001B[1;32m     60\u001B[0m   oneofs\u001B[38;5;241m=\u001B[39m[\n\u001B[1;32m     61\u001B[0m   ],\n\u001B[1;32m     62\u001B[0m   serialized_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m149\u001B[39m,\n\u001B[1;32m     63\u001B[0m   serialized_end\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m182\u001B[39m,\n\u001B[1;32m     64\u001B[0m )\n\u001B[1;32m     66\u001B[0m _TENSORSHAPEPROTO \u001B[38;5;241m=\u001B[39m _descriptor\u001B[38;5;241m.\u001B[39mDescriptor(\n\u001B[1;32m     67\u001B[0m   name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTensorShapeProto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     68\u001B[0m   full_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtensorflow.TensorShapeProto\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    100\u001B[0m   serialized_end\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m182\u001B[39m,\n\u001B[1;32m    101\u001B[0m )\n\u001B[1;32m    103\u001B[0m _TENSORSHAPEPROTO_DIM\u001B[38;5;241m.\u001B[39mcontaining_type \u001B[38;5;241m=\u001B[39m _TENSORSHAPEPROTO\n",
      "File \u001B[0;32m~/anaconda3/envs/tf-gpu-cuda/lib/python3.9/site-packages/google/protobuf/descriptor.py:621\u001B[0m, in \u001B[0;36mFieldDescriptor.__new__\u001B[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001B[0m\n\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__new__\u001B[39m(\u001B[38;5;28mcls\u001B[39m, name, full_name, index, number, \u001B[38;5;28mtype\u001B[39m, cpp_type, label,\n\u001B[1;32m    616\u001B[0m             default_value, message_type, enum_type, containing_type,\n\u001B[1;32m    617\u001B[0m             is_extension, extension_scope, options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    618\u001B[0m             serialized_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    619\u001B[0m             has_default_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, containing_oneof\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, json_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    620\u001B[0m             file\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, create_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):  \u001B[38;5;66;03m# pylint: disable=redefined-builtin\u001B[39;00m\n\u001B[0;32m--> 621\u001B[0m   \u001B[43m_message\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMessage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_CheckCalledFromGeneratedFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    622\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m is_extension:\n\u001B[1;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _message\u001B[38;5;241m.\u001B[39mdefault_pool\u001B[38;5;241m.\u001B[39mFindExtensionByName(full_name)\n",
      "\u001B[0;31mTypeError\u001B[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Overview\n",
    "\n",
    "The code performs the following steps:\n",
    "1. **System Configuration**: Configures CPU/GPU and TensorFlow settings.\n",
    "2. **Data Preprocessing**: Loads and preprocesses Vietnamese news datasets using PhoBERT tokenizer.\n",
    "3. **Model Building**: Constructs a model with a custom PhoBERT layer, CLS token extractor, and dense layers.\n",
    "4. **Training**: Trains the model with early stopping and TensorBoard logging.\n",
    "5. **Evaluation**: Evaluates the model on a test set and generates a classification report.\n",
    "\n",
    "Run the cell below to execute the entire pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T18:13:55.096644Z",
     "start_time": "2025-04-19T18:13:54.697771Z"
    }
   },
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import Dropout, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('fake_news_classifier.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Resource optimization configurations\n",
    "NUM_FOLDS = 3  # Default number of folds, can be adjusted\n",
    "MAX_EPOCHS = 10  # Default max epochs, can be adjusted\n",
    "LOW_MEMORY_THRESHOLD = 8 * 1024  # 8GB in MB\n",
    "\n",
    "# Utility functions for system checks\n",
    "def configure_cpu_threads():\n",
    "    num_physical_cores = os.cpu_count()\n",
    "    num_logical_cores = os.cpu_count()\n",
    "    if num_physical_cores:\n",
    "        os.environ[\"TF_NUM_INTEROP_THREADS\"] = str(num_physical_cores)\n",
    "        os.environ[\"TF_NUM_INTRAOP_THREADS\"] = str(num_logical_cores)\n",
    "        logger.info(f\"CPU Threading configured: {num_physical_cores} physical cores, {num_logical_cores} logical cores\")\n",
    "\n",
    "def check_nvidia_gpu():\n",
    "    try:\n",
    "        output = subprocess.check_output(['nvidia-smi'], stderr=subprocess.STDOUT).decode('utf-8')\n",
    "        return output\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return None\n",
    "\n",
    "def get_gpu_memory():\n",
    "    nvidia_output = check_nvidia_gpu()\n",
    "    if not nvidia_output:\n",
    "        return 0\n",
    "    try:\n",
    "        lines = nvidia_output.split('\\n')\n",
    "        for line in lines:\n",
    "            if 'MiB' in line and '/' in line:\n",
    "                used, total = [int(x) for x in line.split('|')[3].strip().split('MiB')[0].split('/')]\n",
    "                return total\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error parsing GPU memory: {e}\")\n",
    "        return 0\n",
    "\n",
    "def check_cuda_installation():\n",
    "    try:\n",
    "        subprocess.check_output(['nvcc', '--version'], stderr=subprocess.STDOUT).decode('utf-8')\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "def configure_tensorflow():\n",
    "    gpu_memory = get_gpu_memory()\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    is_low_memory = gpu_memory < LOW_MEMORY_THRESHOLD or not gpu_devices\n",
    "\n",
    "    if gpu_devices and not is_low_memory:\n",
    "        try:\n",
    "            for gpu_device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(gpu_device, True)\n",
    "            tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "            logger.info(f\"Detected {len(gpu_devices)} GPU(s) with {gpu_memory}MB memory: {[gpu_device.name for gpu_device in gpu_devices]}\")\n",
    "        except RuntimeError as runtime_error:\n",
    "            logger.error(f\"GPU error: {runtime_error}\")\n",
    "    else:\n",
    "        logger.warning(f\"Low memory GPU ({gpu_memory}MB) or no GPU detected. Running on CPU or with reduced settings.\")\n",
    "        tf.keras.mixed_precision.set_global_policy(\"float32\")\n",
    "\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    logger.info(\"XLA JIT compilation enabled\")\n",
    "    return is_low_memory\n",
    "\n",
    "# Preprocessing function with better error handling\n",
    "def preprocess_data(texts, labels, text_tokenizer, max_length=256):\n",
    "    if len(texts) == 0:\n",
    "        raise ValueError(\"Empty texts provided for preprocessing\")\n",
    "\n",
    "    texts_list = texts.tolist() if hasattr(texts, 'tolist') else list(texts)\n",
    "    labels_list = labels.tolist() if hasattr(labels, 'tolist') else list(labels)\n",
    "\n",
    "    # Filter out invalid entries\n",
    "    valid_data = [(str(text), label) for text, label in zip(texts_list, labels_list)\n",
    "                  if text and not (isinstance(text, float) and pd.isna(text))]\n",
    "\n",
    "    if not valid_data:\n",
    "        raise ValueError(\"No valid text entries found after filtering\")\n",
    "\n",
    "    cleaned_texts, cleaned_labels = zip(*valid_data)\n",
    "    invalid_count = len(texts_list) - len(valid_data)\n",
    "    if invalid_count > 0:\n",
    "        logger.warning(f\"Removed {invalid_count} invalid text entries during preprocessing\")\n",
    "\n",
    "    # Process in batches to avoid memory issues with large datasets\n",
    "    batch_size = 1000\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "\n",
    "    for i in range(0, len(cleaned_texts), batch_size):\n",
    "        batch_texts = list(cleaned_texts[i:i+batch_size])\n",
    "        tokenized_inputs = text_tokenizer(batch_texts, padding='max_length', truncation=True,\n",
    "                                        max_length=max_length, return_tensors='tf')\n",
    "        all_input_ids.append(tokenized_inputs['input_ids'])\n",
    "        all_attention_masks.append(tokenized_inputs['attention_mask'])\n",
    "\n",
    "    return (tf.concat(all_input_ids, axis=0),\n",
    "            tf.concat(all_attention_masks, axis=0),\n",
    "            tf.convert_to_tensor(cleaned_labels, dtype=tf.float32))\n",
    "\n",
    "# Function to determine optimal max_length\n",
    "def determine_max_length(texts, tokenizer, sample_size=1000, is_low_memory=False):\n",
    "    \"\"\"\n",
    "    Determines optimal max_length based on tokenized text lengths.\n",
    "    Returns a value between 128 and 512, or 128 for low memory.\n",
    "    \"\"\"\n",
    "    if is_low_memory:\n",
    "        logger.info(\"Low memory detected, using max_length=128\")\n",
    "        return 128\n",
    "\n",
    "    sample_texts = texts.sample(min(sample_size, len(texts))).tolist()\n",
    "    lengths = [len(tokenizer.encode(text, add_special_tokens=True)) for text in sample_texts]\n",
    "    avg_length = int(sum(lengths) / len(lengths))\n",
    "    max_length = min(max(128, avg_length + 50), 512)  # Add padding, cap at 512\n",
    "    logger.info(f\"Calculated max_length: {max_length} (based on average token length: {avg_length})\")\n",
    "    return max_length\n",
    "\n",
    "# Function to get embeddings for SMOTE\n",
    "def get_embeddings(texts, tokenizer, model, max_length=256):\n",
    "    input_ids, attention_mask, _ = preprocess_data(texts, [0]*len(texts), tokenizer, max_length=max_length)\n",
    "    embeddings = model([input_ids, attention_mask])[0][:, 0, :].numpy()  # CLS token\n",
    "    return embeddings\n",
    "\n",
    "# Inference function\n",
    "def predict_news(model, tokenizer, texts, max_length=256):\n",
    "    \"\"\"\n",
    "    Predicts whether input text(s) are fake news.\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        tokenizer: Transformer tokenizer\n",
    "        texts: Single string or list of strings\n",
    "        max_length: Max token length\n",
    "    Returns:\n",
    "        List of dicts with text, probability, and label\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    input_ids, attention_mask, _ = preprocess_data(texts, [0]*len(texts), tokenizer, max_length=max_length)\n",
    "    predictions = model.predict([input_ids, attention_mask], verbose=0)\n",
    "    results = []\n",
    "\n",
    "    for text, prob in zip(texts, predictions.flatten()):\n",
    "        label = \"Fake\" if prob > 0.5 else \"Real\"\n",
    "        results.append({\n",
    "            \"text\": text[:100] + \"...\" if len(text) > 100 else text,\n",
    "            \"probability_fake\": float(prob),\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Custom layers\n",
    "class CustomTransformerLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, model_name=\"vinai/phobert-base\", trainable_layers=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model_name = model_name\n",
    "        self.transformer = TFAutoModel.from_pretrained(model_name)\n",
    "        self.trainable_layers = trainable_layers\n",
    "        logger.info(f\"Transformer model initialized: {model_name}\")\n",
    "        # Fine-tune only the last few layers\n",
    "        for layer in self.transformer.layers[:-trainable_layers]:\n",
    "            layer.trainable = False\n",
    "        for layer in self.transformer.layers[-trainable_layers:]:\n",
    "            layer.trainable = True\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs_data, **kwargs):\n",
    "        input_ids_data, attention_mask_data = inputs_data\n",
    "        return self.transformer(input_ids=input_ids_data, attention_mask=attention_mask_data, **kwargs)[0]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"model_name\": self.model_name, \"trainable_layers\": self.trainable_layers})\n",
    "        return config\n",
    "\n",
    "class CLSTokenExtractor(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, input_data):\n",
    "        return input_data[:, 0, :]\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "# Model building function\n",
    "def build_model(dropout_rate=0.4, learning_rate=1e-5, model_name=\"vinai/phobert-base\"):\n",
    "    \"\"\"\n",
    "    Builds model with specified transformer (PhoBERT or BERT multilingual).\n",
    "    model_name: 'vinai/phobert-base' or 'bert-base-multilingual-cased'\n",
    "    \"\"\"\n",
    "    input_ids_data = Input(shape=(256,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask_data = Input(shape=(256,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    transformer_layer = CustomTransformerLayer(model_name=model_name, trainable_layers=2)\n",
    "    transformer_output = transformer_layer([input_ids_data, attention_mask_data])\n",
    "\n",
    "    cls_token_extractor = CLSTokenExtractor()\n",
    "    text_embedding = cls_token_extractor(transformer_output)\n",
    "\n",
    "    dropout_layer = Dropout(dropout_rate)(text_embedding)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "    built_model = Model(inputs=[input_ids_data, attention_mask_data], outputs=output_layer)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    built_model.compile(optimizer=optimizer, loss='binary_crossentropy',\n",
    "                       metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "    return built_model\n",
    "\n",
    "# Dataset creation with memory optimization\n",
    "def create_dataset(input_ids, attention_masks, labels, batch_size=32, buffer_size=1000, is_large_dataset=False):\n",
    "    \"\"\"\n",
    "    Creates a TensorFlow dataset with specified batch size.\n",
    "    Note: Adjust batch_size and buffer_size based on dataset size and hardware.\n",
    "    \"\"\"\n",
    "    if is_large_dataset:\n",
    "        buffer_size = min(buffer_size, 500)  # Reduce buffer for large datasets\n",
    "        logger.info(f\"Large dataset detected, reduced buffer_size to {buffer_size}\")\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        ({'input_ids': input_ids, 'attention_mask': attention_masks}, labels)\n",
    "    )\n",
    "    return dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Model saving and loading with proper directory creation\n",
    "def save_model(model_to_save, path='./model/fake_news_model.keras'):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        model_to_save.save(path, save_format='keras')\n",
    "        logger.info(f\"Model saved successfully to {path}\")\n",
    "    except Exception as save_error:\n",
    "        logger.error(f\"Error saving model: {save_error}\")\n",
    "        weights_path = path.replace('.keras', '.weights.h5')\n",
    "        model_to_save.save_weights(weights_path)\n",
    "        logger.info(f\"Model weights saved to {weights_path}\")\n",
    "\n",
    "def load_model(path='./model/fake_news_model.keras'):\n",
    "    custom_objects = {\n",
    "        'CustomTransformerLayer': CustomTransformerLayer,\n",
    "        'CLSTokenExtractor': CLSTokenExtractor\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        return tf.keras.models.load_model(path, custom_objects=custom_objects)\n",
    "    except Exception as load_error:\n",
    "        logger.error(f\"Error loading model: {load_error}\")\n",
    "        weights_path = path.replace('.keras', '.weights.h5')\n",
    "        if os.path.exists(weights_path):\n",
    "            loaded_model = build_model()\n",
    "            loaded_model.load_weights(weights_path)\n",
    "            logger.info(f\"Model weights loaded from {weights_path}\")\n",
    "            return loaded_model\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Neither model nor weights found at {path}\")\n",
    "\n",
    "# Main execution\n",
    "configure_cpu_threads()\n",
    "logger.info(\"CHECKING GPU PREREQUISITES:\")\n",
    "logger.info(f\"NVIDIA GPU available: {bool(check_nvidia_gpu())}\")\n",
    "logger.info(f\"CUDA installed: {check_cuda_installation()}\")\n",
    "is_low_memory = configure_tensorflow()\n",
    "\n",
    "try:\n",
    "    # Set up logging directory\n",
    "    log_dir = \"./logs/\" + time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    logger.info(\"To visualize training progress, run: tensorboard --logdir=./logs\")\n",
    "\n",
    "    # Load data\n",
    "    logger.info(\"Loading dataset...\")\n",
    "    combined_data = pd.read_csv('./data/vnexpress_combined_dataset.csv')\n",
    "\n",
    "    # Check for NaN values and invalid Labels\n",
    "    if combined_data['Content'].isnull().any() or combined_data['Label'].isnull().any():\n",
    "        logger.error(\"Found NaN values in 'Content' or 'Label' columns\")\n",
    "        raise ValueError(\"Invalid data detected\")\n",
    "    if not combined_data['Label'].isin([0, 1]).all():\n",
    "        logger.error(\"Invalid Label values detected (must be 0 or 1)\")\n",
    "        raise ValueError(\"Invalid Label values\")\n",
    "\n",
    "    # Check label distribution\n",
    "    label_ratio = combined_data['Label'].mean()\n",
    "    is_large_dataset = len(combined_data) > 100000\n",
    "\n",
    "    # Initialize tokenizer for SMOTE and max_length\n",
    "    smote_tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n",
    "    max_length = determine_max_length(combined_data['Content'], smote_tokenizer, is_low_memory=is_low_memory)\n",
    "\n",
    "    if label_ratio < 0.2 or label_ratio > 0.8:\n",
    "        logger.warning(f\"Imbalanced dataset detected: {label_ratio*100:.1f}% fake news. Applying SMOTE to balance data.\")\n",
    "\n",
    "        # Load embedding model for SMOTE\n",
    "        embedding_model = TFAutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "\n",
    "        # Apply SMOTE on embeddings\n",
    "        logger.info(\"Generating embeddings for SMOTE...\")\n",
    "        embeddings = get_embeddings(combined_data['Content'], smote_tokenizer, embedding_model, max_length=max_length)\n",
    "        smote = SMOTE(random_state=42)\n",
    "        embeddings_resampled, labels_resampled = smote.fit_resample(embeddings, combined_data['Label'])\n",
    "\n",
    "        # Create new balanced dataset\n",
    "        balanced_data = []\n",
    "        original_texts = combined_data['Content'].tolist()\n",
    "        for idx in range(len(labels_resampled)):\n",
    "            original_idx = min(idx, len(original_texts)-1)\n",
    "            balanced_data.append({\n",
    "                'Content': original_texts[original_idx],\n",
    "                'Label': labels_resampled[idx]\n",
    "            })\n",
    "        combined_data = pd.DataFrame(balanced_data)\n",
    "        new_label_ratio = combined_data['Label'].mean()\n",
    "        logger.info(f\"After SMOTE: {len(combined_data)} records, {new_label_ratio*100:.1f}% fake news\")\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weight = {0: 1.0, 1: 1.0 / label_ratio if label_ratio > 0.5 else 1.0}\n",
    "    logger.info(f\"Class weights: {class_weight}\")\n",
    "\n",
    "    logger.info(f\"Dataset loaded: {len(combined_data)} records \"\n",
    "                f\"({len(combined_data[combined_data['Label'] == 0])} real, \"\n",
    "                f\"{len(combined_data[combined_data['Label'] == 1])} fake)\")\n",
    "\n",
    "    # Resource optimization\n",
    "    num_folds = NUM_FOLDS\n",
    "    max_epochs = MAX_EPOCHS\n",
    "    batch_size = 32\n",
    "\n",
    "    if is_large_dataset:\n",
    "        num_folds = min(num_folds, 2)  # Reduce folds for large datasets\n",
    "        max_epochs = min(max_epochs, 5)  # Reduce epochs for large datasets\n",
    "        batch_size = 64\n",
    "        logger.info(\"Large dataset detected (>100K samples), reduced num_folds to 2, max_epochs to 5, batch_size to 64\")\n",
    "\n",
    "    if is_low_memory:\n",
    "        batch_size = 8  # Reduce batch size for low memory\n",
    "        max_epochs = min(max_epochs, 5)  # Reduce epochs for low memory\n",
    "        logger.info(\"Low memory detected, reduced batch_size to 8, max_epochs to 5\")\n",
    "\n",
    "    # Multi-model evaluation with cross-validation\n",
    "    model_names = [\"vinai/phobert-base\", \"bert-base-multilingual-cased\"]\n",
    "    model_results = []\n",
    "    tokenizers = {}  # Cache tokenizers for each model\n",
    "    kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for model_name in model_names:\n",
    "        logger.info(f\"\\n=== Training model: {model_name} with {num_folds}-fold cross-validation ===\")\n",
    "\n",
    "        # Load and cache tokenizer\n",
    "        tokenizers[model_name] = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Prepare data\n",
    "        X = combined_data['Content'].values\n",
    "        y = combined_data['Label'].values\n",
    "        fold_results = []\n",
    "        best_fold_auc = 0\n",
    "        best_fold_model_path = None\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "            logger.info(f\"\\n--- Fold {fold_idx + 1}/{num_folds} ---\")\n",
    "\n",
    "            # Split data\n",
    "            train_texts, val_texts = X[train_idx], X[val_idx]\n",
    "            train_labels, val_labels = y[train_idx], y[val_idx]\n",
    "\n",
    "            # Preprocess data\n",
    "            logger.info(\"Preprocessing data...\")\n",
    "            train_input_ids, train_attention_mask, train_labels_data = preprocess_data(train_texts, train_labels, tokenizers[model_name], max_length=max_length)\n",
    "            val_input_ids, val_attention_mask, val_labels_data = preprocess_data(val_texts, val_labels, tokenizers[model_name], max_length=max_length)\n",
    "\n",
    "            # Create datasets\n",
    "            logger.info(\"Creating datasets...\")\n",
    "            train_dataset = create_dataset(train_input_ids, train_attention_mask, train_labels_data, batch_size=batch_size, is_large_dataset=is_large_dataset)\n",
    "            val_dataset = create_dataset(val_input_ids, val_attention_mask, val_labels_data, batch_size=batch_size, is_large_dataset=is_large_dataset)\n",
    "\n",
    "            # Train model\n",
    "            logger.info(\"Building model...\")\n",
    "            model = build_model(dropout_rate=0.4, learning_rate=1e-5, model_name=model_name)\n",
    "\n",
    "            # Set up callbacks\n",
    "            model_dir = f\"./model/{model_name.split('/')[-1]}/fold_{fold_idx + 1}\"\n",
    "            os.makedirs(model_dir, exist_ok=True)\n",
    "            callbacks = [\n",
    "                EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                ModelCheckpoint(f\"{model_dir}/checkpoint_loss.keras\", monitor='val_loss', save_best_only=True),\n",
    "                ModelCheckpoint(f\"{model_dir}/checkpoint_auc.keras\", monitor='val_auc', save_best_only=True, mode='max'),\n",
    "                TensorBoard(log_dir=os.path.join(log_dir, f\"{model_name.split('/')[-1]}/fold_{fold_idx + 1}\"), histogram_freq=1),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "            ]\n",
    "\n",
    "            logger.info(\"Training model...\")\n",
    "            start_time = time.time()\n",
    "            history = model.fit(\n",
    "                train_dataset,\n",
    "                validation_data=val_dataset,\n",
    "                epochs=max_epochs,\n",
    "                callbacks=callbacks,\n",
    "                class_weight=class_weight\n",
    "            )\n",
    "            training_time = time.time() - start_time\n",
    "            logger.info(f\"Total training time: {training_time:.2f} seconds\")\n",
    "\n",
    "            # Save training history\n",
    "            history_path = os.path.join(log_dir, f\"training_history_{model_name.split('/')[-1]}_fold_{fold_idx + 1}_{time.strftime('%Y%m%d-%H%M%S')}.json\")\n",
    "            with open(history_path, 'w') as f:\n",
    "                json.dump(history.history, f)\n",
    "            logger.info(f\"Training history saved to {history_path}\")\n",
    "\n",
    "            # Evaluate model\n",
    "            logger.info(\"Evaluating model...\")\n",
    "            val_results = model.evaluate(val_dataset, verbose=1)\n",
    "            logger.info(f\"Validation Loss: {val_results[0]:.4f}\")\n",
    "            logger.info(f\"Validation Accuracy: {val_results[1]:.4f}\")\n",
    "            logger.info(f\"Validation AUC: {val_results[2]:.4f}\")\n",
    "\n",
    "            # Predict and compute metrics\n",
    "            logger.info(\"Generating predictions for detailed evaluation...\")\n",
    "            predictions = model.predict(val_dataset)\n",
    "            predicted_labels = (predictions > 0.5).astype(int).flatten()\n",
    "            true_labels = val_labels_data.numpy().flatten()\n",
    "\n",
    "            cm = confusion_matrix(true_labels, predicted_labels)\n",
    "            logger.info(f\"Confusion Matrix:\\n{cm}\")\n",
    "            report = classification_report(true_labels, predicted_labels, target_names=['Real', 'Fake'], output_dict=True)\n",
    "            logger.info(f\"\\nClassification Report:\\n{classification_report(true_labels, predicted_labels, target_names=['Real', 'Fake'])}\")\n",
    "\n",
    "            # Compute F1-score and AUC\n",
    "            f1 = f1_score(true_labels, predicted_labels)\n",
    "            auc = roc_auc_score(true_labels, predictions.flatten())\n",
    "\n",
    "            # Save model\n",
    "            model_path = f\"{model_dir}/fake_news_model.keras\"\n",
    "            save_model(model, model_path)\n",
    "\n",
    "            # Store fold results\n",
    "            fold_results.append({\n",
    "                \"fold\": fold_idx + 1,\n",
    "                \"val_loss\": float(val_results[0]),\n",
    "                \"val_accuracy\": float(val_results[1]),\n",
    "                \"val_auc\": float(val_results[2]),\n",
    "                \"f1_score\": float(f1),\n",
    "                \"classification_report\": report,\n",
    "                \"model_path\": model_path\n",
    "            })\n",
    "\n",
    "            # Update best fold\n",
    "            if auc > best_fold_auc:\n",
    "                best_fold_auc = auc\n",
    "                best_fold_model_path = model_path\n",
    "\n",
    "        # Compute average metrics\n",
    "        avg_results = {\n",
    "            \"model_name\": model_name,\n",
    "            \"avg_val_loss\": float(np.mean([r[\"val_loss\"] for r in fold_results])),\n",
    "            \"avg_val_accuracy\": float(np.mean([r[\"val_accuracy\"] for r in fold_results])),\n",
    "            \"avg_val_auc\": float(np.mean([r[\"val_auc\"] for r in fold_results])),\n",
    "            \"avg_f1_score\": float(np.mean([r[\"f1_score\"] for r in fold_results])),\n",
    "            \"std_val_auc\": float(np.std([r[\"val_auc\"] for r in fold_results])),\n",
    "            \"folds\": fold_results,\n",
    "            \"best_fold_model_path\": best_fold_model_path\n",
    "        }\n",
    "        model_results.append(avg_results)\n",
    "\n",
    "        # Log fold results\n",
    "        logger.info(f\"\\nSummary for {model_name}:\")\n",
    "        logger.info(f\"Average Validation Loss: {avg_results['avg_val_loss']:.4f}\")\n",
    "        logger.info(f\"Average Validation Accuracy: {avg_results['avg_val_accuracy']:.4f}\")\n",
    "        logger.info(f\"Average Validation AUC: {avg_results['avg_val_auc']:.4f}\")\n",
    "        logger.info(f\"Average F1-score: {avg_results['avg_f1_score']:.4f}\")\n",
    "        logger.info(f\"Standard Deviation AUC: {avg_results['std_val_auc']:.4f}\")\n",
    "        logger.info(f\"Best fold model saved at: {best_fold_model_path}\")\n",
    "\n",
    "    # Save model comparison results\n",
    "    comparison_path = os.path.join(log_dir, f\"cross_validation_comparison_{time.strftime('%Y%m%d-%H%M%S')}.json\")\n",
    "    with open(comparison_path, 'w') as f:\n",
    "        json.dump(model_results, f, indent=2)\n",
    "    logger.info(f\"Cross-validation comparison results saved to {comparison_path}\")\n",
    "\n",
    "    # Log best model\n",
    "    best_model = max(model_results, key=lambda x: x[\"avg_val_auc\"])\n",
    "    logger.info(f\"\\nBest model based on average AUC: {best_model['model_name']}\")\n",
    "    logger.info(f\"Average Validation AUC: {best_model['avg_val_auc']:.4f}\")\n",
    "    logger.info(f\"Average F1-score: {best_model['avg_f1_score']:.4f}\")\n",
    "    logger.info(f\"Best fold model saved at: {best_model['best_fold_model_path']}\")\n",
    "\n",
    "    # Example inference\n",
    "    logger.info(\"\\nTesting inference on sample text...\")\n",
    "    sample_texts = [\n",
    "        \"Chnh ph Vit Nam cng b k hoch pht trin nng lng ti to n nm 2030.\",\n",
    "        \"C mp bit bay xut hin  H Ni, gy hoang mang d lun.\"\n",
    "    ]\n",
    "    best_model_instance = load_model(best_model[\"best_fold_model_path\"])\n",
    "    best_tokenizer = tokenizers[best_model[\"model_name\"]]\n",
    "    predictions = predict_news(best_model_instance, best_tokenizer, sample_texts, max_length=max_length)\n",
    "    for pred in predictions:\n",
    "        logger.info(f\"Text: {pred['text']}\")\n",
    "        logger.info(f\"Probability Fake: {pred['probability_fake']:.4f}\")\n",
    "        logger.info(f\"Label: {pred['label']}\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in execution: {e}\")\n",
    "    import traceback\n",
    "    logger.error(traceback.format_exc())"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m confusion_matrix, classification_report\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_test_split\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dropout, Dense, Input\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Model\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EarlyStopping, ModelCheckpoint, TensorBoard\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/api/_v2/keras/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_v2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __internal__\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_v2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_v2\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __internal__\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m activations\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/__internal__/__init__.py:6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layers\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m losses\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m optimizers\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m__internal__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/__internal__/models/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcloning\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m clone_and_build_model\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcloning\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m in_place_subclassed_model_state_restoration\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/src/__init__.py:21\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03mDetailed documentation and user guides are available at\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m[keras.io](https://keras.io).\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m applications\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/src/applications/__init__.py:18\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# limitations under the License.\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconvnext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConvNeXtBase\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconvnext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConvNeXtLarge\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconvnext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConvNeXtSmall\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/src/applications/convnext.py:33\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m utils\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapplications\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m imagenet_utils\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sequential\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m training \u001B[38;5;28;01mas\u001B[39;00m training_lib\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# isort: off\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/src/engine/sequential.py:24\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layers \u001B[38;5;28;01mas\u001B[39;00m layer_module\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m base_layer\n\u001B[0;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m input_layer\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m training\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/src/engine/functional.py:33\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m input_spec\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m node \u001B[38;5;28;01mas\u001B[39;00m node_module\n\u001B[0;32m---> 33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m training \u001B[38;5;28;01mas\u001B[39;00m training_lib\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m training_utils\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialization_lib\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/src/engine/training.py:48\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptimizers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m optimizer_v1\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pickle_utils\n\u001B[0;32m---> 48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m saving_api\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m saving_lib\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialization_lib\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/src/saving/saving_api.py:25\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtf_export\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras_export\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m saving_lib\n\u001B[0;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m save \u001B[38;5;28;01mas\u001B[39;00m legacy_sm_saving_lib\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m io_utils\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/src/saving/legacy/save.py:27\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m serialization\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaved_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load \u001B[38;5;28;01mas\u001B[39;00m saved_model_load\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaved_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_context\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaved_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m save \u001B[38;5;28;01mas\u001B[39;00m saved_model_save\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaving\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlegacy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msaved_model\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras_option_scope\n",
      "File \u001B[0;32m~/anaconda3/envs/Homeword/lib/python3.12/site-packages/keras/src/saving/legacy/saved_model/load_context.py:68\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns whether under a load context.\"\"\"\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _load_context\u001B[38;5;241m.\u001B[39min_load_context()\n\u001B[0;32m---> 68\u001B[0m tf\u001B[38;5;241m.\u001B[39m__internal__\u001B[38;5;241m.\u001B[39mregister_load_context_function(in_load_context)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'tensorflow._api.v2.compat.v2.__internal__' has no attribute 'register_load_context_function'"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Results\n",
    "\n",
    "- **Logs**: Check the `fake_news_classifier.log` file in the Colab file explorer for detailed logs, including `Test Accuracy`, `Test AUC`, and `Classification Report`.\n",
    "- **TensorBoard**: Run the cell below to visualize training metrics (loss, accuracy, AUC) in TensorBoard.\n",
    "- **Model Files**: The trained model is saved as `./model/fake_news_model.keras` or weights as `./model/fake_news_model.weights.h5`.\n",
    "\n",
    "### Launch TensorBoard\n",
    "Run the following cell to start TensorBoard and view training progress."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-19T18:01:02.631574Z",
     "start_time": "2025-04-19T18:00:58.551945Z"
    }
   },
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-962371ac651b467f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-962371ac651b467f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
