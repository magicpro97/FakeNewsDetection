from sklearn.metrics import classification_report
import pandas as pd
import numpy as np
import re
import string
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
import matplotlib.pyplot as plt
import requests


# T·∫£i danh s√°ch stopwords t·ª´ VietAI GitHub
url = "https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords.txt"
stop_words_vietnamese = set(requests.get(url).text.split("\n"))

# ƒê·ªçc d·ªØ li·ªáu tin th·∫≠t
df_real = pd.read_csv("./data/vnexpress_dataset.csv")
df_real["label"] = 0  # 0: Tin th·∫≠t

# ƒê·ªçc d·ªØ li·ªáu tin gi·∫£
df_fake = pd.read_csv("./data/vnexpress_fake_dataset.csv")
df_fake["label"] = 1  # 1: Tin gi·∫£

# G·ªôp hai b·ªô d·ªØ li·ªáu
df = pd.concat([df_real, df_fake], ignore_index=True)

# X·ª≠ l√Ω n·ªôi dung tr·ªëng
df = df.dropna(subset=["Content"])


def clean_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'\s+', ' ', text).strip()
    words = text.split()
    words = [word for word in words if word not in stop_words_vietnamese]
    return " ".join(words)


df["Content"] = df["Content"].apply(clean_text)

# Chia d·ªØ li·ªáu train/test
X_train, X_temp, y_train, y_temp = train_test_split(
    df["Content"], df["label"], test_size=0.3, random_state=42, stratify=df["label"])

X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)


# Tokenization v·ªõi LSTM (Deep Learning)
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)  # Ch·ªâ fit tr√™n t·∫≠p train

# Chuy·ªÉn vƒÉn b·∫£n th√†nh chu·ªói s·ªë
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Padding
max_length = 500
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding="post")
X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding="post")
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding="post")

# Deep Learning v·ªõi LSTM
model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=max_length),
    # L2 Regularization
    Bidirectional(LSTM(64, return_sequences=True,
                  kernel_regularizer=l2(0.01))),
    Bidirectional(LSTM(32, kernel_regularizer=l2(0.01))),
    Dropout(0.5),
    # Th√™m m·ªôt hidden layer ƒë·ªÉ h·ªçc t·ªët h∆°n
    Dense(16, activation="relu", kernel_regularizer=l2(0.01)),
    Dense(1, activation="sigmoid")
])

model.compile(loss="binary_crossentropy",
              optimizer="adam", metrics=["accuracy"])

early_stopping = EarlyStopping(
    monitor='val_loss', patience=3, restore_best_weights=True)

# D√πng Early Stopping ƒë·ªÉ m√¥ h√¨nh t·ª± ƒë·ªông d·ª´ng khi kh√¥ng c√≤n c·∫£i thi·ªán.
# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
history = model.fit(X_train_pad, y_train, epochs=10, batch_size=64,
          validation_data=(X_val_pad, y_val))


# L∆∞u m√¥ h√¨nh d∆∞·ªõi d·∫°ng file .keras
model.save("./model/fake_news_model_LSMT.keras")

# Ki·ªÉm tra m√¥ h√¨nh
y_pred_prob = model.predict(X_test_pad)
y_pred = (y_pred_prob > 0.5).astype(int)
print(classification_report(y_test, y_pred))


# D·ª± ƒëo√°n tin gi·∫£
def predict_fake_news(text):
    text = clean_text(text)
    text_seq = tokenizer.texts_to_sequences([text])
    text_pad = pad_sequences(text_seq, maxlen=max_length, padding="post")

    prob = model.predict(text_pad)[0][0]  # X√°c su·∫•t tin gi·∫£

    label = "üõë Tin gi·∫£" if prob > 0.5 else "‚úÖ Tin th·∫≠t"
    confidence = f"{prob*100:.2f}%" if prob > 0.5 else f"{(1-prob)*100:.2f}%"

    return f"{label} (Confidence: {confidence})"


# V√≠ d·ª• ki·ªÉm tra tin t·ª©c
news_real = f"Th·ªß t∆∞·ªõng Ph·∫°m Minh Ch√≠nh cho bi·∫øt Vi·ªát Nam lu√¥n coi tr·ªçng h·ª£p t√°c v·ªõi EU v√† ƒë·ªÅ ngh·ªã EU s·ªõm ho√†n t·∫•t ph√™ chu·∫©n Hi·ªáp ƒë·ªãnh B·∫£o h·ªô ƒë·∫ßu t∆∞ Vi·ªát Nam-EU."
news_fake = f"C√¥ng ty c·ªßa di·ªÖn vi√™n h√†i Huy Pham ph·∫£i tr·∫£ 15 t·ª∑ ƒë·ªìng cho ƒë·ªëi t√°c"

print(predict_fake_news(news_real))
print(predict_fake_news(news_fake))

# L·∫•y d·ªØ li·ªáu loss v√† accuracy t·ª´ history
train_loss = history.history["loss"]
val_loss = history.history["val_loss"]
train_acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]

epochs = range(1, len(train_loss) + 1)

# V·∫Ω Loss Curve
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss, "b", label="Training Loss")
plt.plot(epochs, val_loss, "r", label="Validation Loss")
plt.title("Loss Curve")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

# V·∫Ω Accuracy Curve
plt.subplot(1, 2, 2)
plt.plot(epochs, train_acc, "b", label="Training Accuracy")
plt.plot(epochs, val_acc, "r", label="Validation Accuracy")
plt.title("Accuracy Curve")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

plt.show()
