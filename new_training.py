from sklearn.metrics import classification_report
import pandas as pd
import numpy as np
import re
import string
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# ƒê·ªçc d·ªØ li·ªáu tin th·∫≠t
df_real = pd.read_csv("./data/vnexpress_dataset.csv")
df_real["label"] = 0  # 0: Tin th·∫≠t

# ƒê·ªçc d·ªØ li·ªáu tin gi·∫£
df_fake = pd.read_csv("./data/vnexpress_fake_dataset.csv")
df_fake["label"] = 1  # 1: Tin gi·∫£

# G·ªôp hai b·ªô d·ªØ li·ªáu
df = pd.concat([df_real, df_fake], ignore_index=True)

# X·ª≠ l√Ω n·ªôi dung tr·ªëng
df = df.dropna(subset=["Content"])


def clean_text(text):
    text = text.lower()  # Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    text = re.sub(r'\d+', '', text)  # Lo·∫°i b·ªè s·ªë
    text = text.translate(str.maketrans(
        '', '', string.punctuation))  # Lo·∫°i b·ªè d·∫•u c√¢u
    text = re.sub(r'\s+', ' ', text).strip()  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    return text


df["Content"] = df["Content"].apply(clean_text)

# Chia d·ªØ li·ªáu train/test
X_train, X_test, y_train, y_test = train_test_split(
    df["Content"], df["label"], test_size=0.2, random_state=42, stratify=df["label"])

tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)

# Tokenization v·ªõi LSTM (Deep Learning)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

max_length = 200  # Gi·ªõi h·∫°n ƒë·ªô d√†i c·ªßa m·ªói vƒÉn b·∫£n
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding="post")
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding="post")

# Deep Learning v·ªõi LSTM

model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=max_length),
    LSTM(64, return_sequences=True),
    LSTM(32),
    Dropout(0.5),
    Dense(1, activation="sigmoid")
])

model.compile(loss="binary_crossentropy",
              optimizer="adam", metrics=["accuracy"])

# Hu·∫•n luy·ªán m√¥ h√¨nh
model.fit(X_train_pad, y_train, epochs=5, batch_size=32,
          validation_data=(X_test_pad, y_test))

# Ki·ªÉm tra m√¥ h√¨nh

y_pred_prob = model.predict(X_test_pad)
y_pred = (y_pred_prob > 0.5).astype(int)

print(classification_report(y_test, y_pred))

# D·ª± ƒëo√°n tin gi·∫£


def predict_fake_news(text):
    text = clean_text(text)
    text_seq = tokenizer.texts_to_sequences([text])
    text_pad = pad_sequences(text_seq, maxlen=max_length, padding="post")

    prob = model.predict(text_pad)[0][0]  # X√°c su·∫•t tin gi·∫£

    label = "üõë Tin gi·∫£" if prob > 0.5 else "‚úÖ Tin th·∫≠t"
    confidence = f"{prob*100:.2f}%" if prob > 0.5 else f"{(1-prob)*100:.2f}%"

    return f"{label} (Confidence: {confidence})"


# V√≠ d·ª• ki·ªÉm tra tin t·ª©c
news_real = f"Th·ªß t∆∞·ªõng Ph·∫°m Minh Ch√≠nh cho bi·∫øt Vi·ªát Nam lu√¥n coi tr·ªçng h·ª£p t√°c v·ªõi EU v√† ƒë·ªÅ ngh·ªã EU s·ªõm ho√†n t·∫•t ph√™ chu·∫©n Hi·ªáp ƒë·ªãnh B·∫£o h·ªô ƒë·∫ßu t∆∞ Vi·ªát Nam-EU."
news_fake = f"Nh√† khoa h·ªçc Vi·ªát Nam ch·∫ø t·∫°o th√†nh c√¥ng m√°y ph√°t ƒëi·ªán vƒ©nh c·ª≠u kh√¥ng c·∫ßn nhi√™n li·ªáu, c√≥ th·ªÉ cung c·∫•p ƒëi·ªán mi·ªÖn ph√≠ cho to√†n b·ªô ƒë·∫•t n∆∞·ªõc."

print(predict_fake_news(news_real))
print(predict_fake_news(news_fake))
